# chat/admin.py

from django.contrib import admin
from .models import HistoricalFigure, Conversation, Message

@admin.register(HistoricalFigure)
class HistoricalFigureAdmin(admin.ModelAdmin):
    list_display = ('name',)

@admin.register(Conversation)
class ConversationAdmin(admin.ModelAdmin):
    list_display = ('user', 'historical_figure', 'created_at')

@admin.register(Message)
class MessageAdmin(admin.ModelAdmin):
    list_display = ('conversation', 'sender', 'timestamp')
from django.apps import AppConfig

class ChatConfig(AppConfig):
    default_auto_field = 'django.db.models.BigAutoField'
    name = 'chat'
# Generated by Django 4.2.17 on 2025-01-11 07:20

from django.conf import settings
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='Conversation',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
            ],
        ),
        migrations.CreateModel(
            name='HistoricalFigure',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(max_length=100, unique=True)),
                ('prompt', models.TextField()),
            ],
        ),
        migrations.CreateModel(
            name='Message',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('sender', models.CharField(max_length=50)),
                ('content', models.TextField()),
                ('timestamp', models.DateTimeField(auto_now_add=True)),
                ('conversation', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='messages', to='chat.conversation')),
            ],
            options={
                'ordering': ['timestamp'],
            },
        ),
        migrations.AddField(
            model_name='conversation',
            name='historical_figure',
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='chat.historicalfigure'),
        ),
        migrations.AddField(
            model_name='conversation',
            name='user',
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='conversations', to=settings.AUTH_USER_MODEL),
        ),
    ]
# Generated by Django 4.2.17 on 2025-01-11 07:50

from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('chat', '0001_initial'),
    ]

    operations = [
        migrations.AlterModelOptions(
            name='message',
            options={},
        ),
        migrations.AlterField(
            model_name='conversation',
            name='historical_figure',
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='conversations', to='chat.historicalfigure'),
        ),
        migrations.AlterField(
            model_name='historicalfigure',
            name='name',
            field=models.CharField(max_length=255),
        ),
        migrations.AlterField(
            model_name='historicalfigure',
            name='prompt',
            field=models.TextField(help_text='系统消息，用于引导AI角色'),
        ),
        migrations.AlterField(
            model_name='message',
            name='sender',
            field=models.CharField(choices=[('user', '用户'), ('assistant', 'AI')], max_length=10),
        ),
    ]
# chat/models.py

from django.db import models
from django.contrib.auth.models import User

class HistoricalFigure(models.Model):
    name = models.CharField(max_length=255)
    prompt = models.TextField(help_text="系统消息，用于引导AI角色")

    def __str__(self):
        return self.name

class Conversation(models.Model):
    user = models.ForeignKey(User, on_delete=models.CASCADE, related_name='conversations')
    historical_figure = models.ForeignKey(HistoricalFigure, on_delete=models.CASCADE, related_name='conversations')
    created_at = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return f"{self.user.username} - {self.historical_figure.name} - {self.created_at.strftime('%Y-%m-%d %H:%M')}"

class Message(models.Model):
    SENDERS = (
        ('user', '用户'),
        ('assistant', 'AI'),
    )
    conversation = models.ForeignKey(Conversation, on_delete=models.CASCADE, related_name='messages')
    sender = models.CharField(max_length=10, choices=SENDERS)
    content = models.TextField()
    timestamp = models.DateTimeField(auto_now_add=True)

    def __str__(self):
        return f"{self.sender} at {self.timestamp.strftime('%Y-%m-%d %H:%M')}: {self.content[:50]}"
from celery import shared_task
from .utils import ChatService
import asyncio

@shared_task
def get_ai_response_task(messages):
    chat_service = ChatService()
    return asyncio.run(chat_service.get_response(messages))
<!-- historical_chat/templates/base.html -->

<!DOCTYPE html>
<html lang="zh-Hans">
<head>
    <meta charset="UTF-8">
    <title>历史人物聊天</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css">
    <style>
        /* 添加简单的样式 */
        .loader {
            border: 4px solid #f3f3f3;
            border-top: 4px solid #3498db;
            border-radius: 50%;
            width: 20px;
            height: 20px;
            animation: spin 2s linear infinite;
            display: inline-block;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .user-message, .assistant-message {
            padding: 10px;
            border-radius: 5px;
            max-width: 80%;
        }

        .user-message {
            background-color: #d1ecf1;
            align-self: flex-end;
        }

        .assistant-message {
            background-color: #f8d7da;
            align-self: flex-start;
        }
    </style>

</head>
<body>
    <div class="container-fluid mt-4">
        {% if user.is_authenticated %}
            <p>欢迎, {{ user.username }}! <a href="{% url 'logout' %}">登出</a></p>
        {% else %}
            <p><a href="{% url 'login' %}">登录</a> | <a href="{% url 'chat:register' %}">注册</a></p>
        {% endif %}
        {% block content %}
        {% endblock %}
    </div>
</body>
</html>
{% extends "base.html" %}

{% block content %}
<div class="container-fluid mt-4" style="min-height: 100vh; display: flex; flex-direction: column;">
    <div class="row" style="flex: 1;">
        <!-- 左侧功能区 -->
        <div class="col-md-3 d-flex flex-column" style="height: 100vh; padding-top: 20px;">
            <!-- 选择历史人物容器 -->
            <div class="card mb-5">
                <div class="card-body">
                    <h5 class="card-title">选择历史人物</h5>
                    <select id="figure" class="form-select mb-3" style="
                        padding: 12px;
                        border-radius: 8px;
                        border: 1px solid #ced4da;
                        background-color: #f8f9fa;
                        font-size: 16px;
                        transition: all 0.3s ease;
                    ">
                        {% for figure in historical_figures %}
                            <option value="{{ figure.id }}">{{ figure.name }}</option>
                        {% endfor %}
                    </select>
                </div>
            </div>

            <!-- 一键清除记忆容器 -->
            <!-- 一键清除记忆容器 -->
            <div class="card mb-5">
                <div class="card-body">
                    <h5 class="card-title">一键清除记忆</h5>
                    <div class="d-flex flex-wrap gap-4 mb-3">
                        {% for figure in historical_figures %}
                            <button class="btn btn-outline-danger btn-sm clear-memory-btn" data-figure-id="{{ figure.id }}" style="flex: 1 0 30%; margin-bottom: 10px;">
                                清除 {{ figure.name }} 的记忆
                            </button>
                        {% endfor %}
                    </div>
                </div>
            </div>

            <!-- 超级思维容器 -->
            <div class="card mb-5">
                <div class="card-body">
                    <h5 class="card-title">超级思维</h5>
                    <button id="superMindBtn" class="btn btn-warning w-100 mb-3">启动超级思维</button>
                </div>
            </div>

            <!-- 隐私保护容器 -->
            <div class="card mb-5">
                <div class="card-body">
                    <h5 class="card-title">隐私保护</h5>
                    <button id="privacyProtectBtn" class="btn btn-info w-100">
                        <i class="fas fa-shield-alt"></i> 隐私保护
                    </button>
                    <small class="text-muted">点击以保护您的隐私数据。</small>
                </div>
            </div>
        </div>


        <!-- 右侧对话区 -->
        <div class="col-md-8 d-flex flex-column">
            <div class="card h-100">
                <div class="card-body d-flex flex-column">
                    <h5 class="card-title" id="conversationTitle">与历史人物对话</h5> <!-- 添加ID -->

                    <!-- 输入框区域 -->
                    <div class="input-group mb-3">
                        <input type="text" id="question" class="form-control" placeholder="请输入你的问题..." />
                        <button id="askBtn" class="btn btn-primary">提问</button>
                    </div>

                    <!-- 加载动画 -->
                    <div id="loading" class="text-center my-3" style="display: none;">
                        <div class="spinner-border text-primary" role="status">
                            <span class="visually-hidden">加载中...</span>
                        </div>
                        <p class="mt-2">AI 正在思考中...</p>
                    </div>

                    <!-- 对话记录 -->
                    <h5 class="card-title mt-4">对话记录</h5>
                    <div id="conversation" class="border p-3" style="flex-grow: 1; overflow-y: auto; background-color: #f8f9fa;">
                        <!-- 对话内容将动态加载 -->
                    </div>
                </div>
            </div>
    </div>
</div>
</div>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
<script>
    let superMindActive = false; // 是否启用超级思维
    function getResponderName() {
    if (superMindActive) {
        return "超级思维"; // 超级思维模式
    } else {
        const selectedFigure = $('#figure option:selected').text(); // 获取选中的历史人物名称
        return selectedFigure || "AI"; // 默认显示为 AI
    }
}

    $(document).ready(function() {
        // 状态变量：是否启用超级思维
        let isSuperMindEnabled = false;

        // 提问按钮点击事件
        $('#askBtn').click(function() {
            let question = $('#question').val().trim();
            if (!question) {
                alert("请先输入问题");
                return;
            }

            // 显示加载提示
            $('#loading').show();

            // 根据是否启用超级思维，选择不同的 URL 和请求格式
            let url = isSuperMindEnabled
                ? "{% url 'chat:ask_difficult_question' %}"  // 超级思维模式
                : "{% url 'chat:ask_question' %}";  // 普通模式

            // 构造请求数据
            let requestData;
            let contentType;
            if (isSuperMindEnabled) {
                // 超级思维模式：发送 JSON 格式
                requestData = JSON.stringify({
                    question: question
                });
                contentType = 'application/json';
            } else {
                // 普通模式：发送表单格式
                requestData = {
                    question: question,
                    figure_id: $('#figure').val(),  // 发送选择的历史人物 ID
                };
                contentType = 'application/x-www-form-urlencoded';
            }

            // 发送 AJAX 请求
            $.ajax({
                url: url,
                type: "POST",
                contentType: contentType,
                data: requestData,
                headers: {
                    'X-CSRFToken': '{{ csrf_token }}'  // 确保传递 CSRF 令牌
                },
                success: function(response) {
                    console.log("AJAX response:", response); // 添加此行以查看响应内容

                    // 隐藏加载提示
                    $('#loading').hide();

                    if (response.status === 'success') {
                        let answer = response.response;
                        // 添加用户的问题
                        $('#conversation').append(
                            `<div class="d-flex justify-content-end mb-2">
                                <div class="user-message">
                                    <strong>你：</strong> ${question}
                                </div>
                            </div>`
                        );
                        // 添加AI的回答
                        const responderName = getResponderName(); // 获取回答者名称
                        $('#conversation').append(
                            `<div class="d-flex justify-content-start mb-2">
                                <div class="assistant-message">
                                    <strong>${responderName}：</strong> ${answer}
                                </div>
                            </div>`
                        );
                        // 清空输入框
                        $('#question').val('');
                        // 自动滚动到最底部
                        $('#conversation').scrollTop($('#conversation')[0].scrollHeight);
                    } else {
                        $('#conversation').append(
                            `<div class="text-danger"><strong>错误：</strong> ${response.message || "未知错误"}</div><hr>`
                        );
                    }
                },
                error: function(xhr) {
                    console.log("AJAX error:", xhr); // 添加此行以查看错误内容
                    // 隐藏加载提示
                    $('#loading').hide();

                    $('#conversation').append(
                        `<div class="text-danger"><strong>请求出错：</strong> ${xhr.responseText}</div><hr>`
                    );
                }
            });
        });

        // 处理一键清除记忆按钮点击事件
        $('.clear-memory-btn').click(function() {
            if (!confirm("确定要清除与该历史人物的所有聊天记录吗？")) {
                return;
            }

            let figure_id = $(this).data('figure-id');

            $.ajax({
                url: "{% url 'chat:clear_memory' %}",
                type: "POST",
                data: {
                    figure_id: figure_id,
                    csrfmiddlewaretoken: '{{ csrf_token }}'
                },
                success: function(response) {
                    if (response.status === 'success') {
                        alert(response.message);
                        // 清除前端的聊天记录显示
                        // 如果当前选择的历史人物被清除，则清空对话区
                        let selectedFigureId = $('#figure').val();
                        if (selectedFigureId == figure_id) {
                            $('#conversation').empty();
                        }
                    } else {
                        alert("记忆清除失败：" + (response.message || "未知错误"));
                    }
                },
                error: function(xhr) {
                    console.log("AJAX error:", xhr); // 添加此行以查看错误内容
                    alert("请求出错：" + xhr.responseText);
                }
            });
        });

        // 超级思维按钮点击事件
        const superMindBtn = document.getElementById("superMindBtn");
        superMindBtn.addEventListener("click", function() {
            // 切换超级思维状态
            isSuperMindEnabled = !isSuperMindEnabled;

            // 更新按钮样式和文本
            if (isSuperMindEnabled) {
                superMindBtn.classList.remove("btn-warning");
                superMindBtn.classList.add("btn-success");
                superMindBtn.innerText = "超级思维已启用";
                // 禁用历史人物选择
                $('#figure').prop('disabled', true);
            } else {
                superMindBtn.classList.remove("btn-success");
                superMindBtn.classList.add("btn-warning");
                superMindBtn.innerText = "超级思维";
                // 启用历史人物选择
                $('#figure').prop('disabled', false);
            }
        });
    });
</script>
<script>
    $(document).ready(function() {

        let isPrivacyEnabled = false; // 状态变量，默认未开启隐私保护

        // 隐私保护按钮点击事件
        $('#privacyProtectBtn').click(function() {
            if (isPrivacyEnabled) {
                // 恢复原始状态（关闭隐私保护）
                $(this).html('<i class="fas fa-shield-alt"></i> 隐私保护');
                $(this).removeClass('btn-success').addClass('btn-info');
                isPrivacyEnabled = false;
                console.log("隐私保护已关闭");
            } else {
                // 开启隐私保护
                $(this).html('<i class="fas fa-shield-alt"></i> 已开启隐私保护');
                $(this).removeClass('btn-info').addClass('btn-success');
                isPrivacyEnabled = true;
                console.log("隐私保护已开启");
            }
        });
    });
</script>

<script>
    $(document).ready(function() {
        let isSuperMindEnabled = false; // 状态变量：是否启用超级思维
        const superMindBtn = document.getElementById("superMindBtn");
        const conversationTitle = document.getElementById("conversationTitle"); // 获取标题元素

        // 超级思维按钮点击事件
        superMindBtn.addEventListener("click", function() {
            // 切换超级思维状态
            isSuperMindEnabled = !isSuperMindEnabled;

            // 更新按钮样式和文本
            if (isSuperMindEnabled) {
                superMindBtn.classList.remove("btn-warning");
                superMindBtn.classList.add("btn-success");
                superMindBtn.innerText = "超级思维已启用";
                $('#figure').prop('disabled', true); // 禁用历史人物选择
                conversationTitle.innerText = "超级思维模式"; // 修改标题
            } else {
                superMindBtn.classList.remove("btn-success");
                superMindBtn.classList.add("btn-warning");
                superMindBtn.innerText = "超级思维";
                $('#figure').prop('disabled', false); // 启用历史人物选择
                conversationTitle.innerText = "与历史人物对话"; // 恢复标题
            }
        });
    });
</script>

{% endblock %}  <!-- 确保这里闭合了 block --><!-- chat/templates/chat/register.html -->

{% extends "base.html" %}

{% block content %}
  <h2>注册</h2>
  <form method="post">
    {% csrf_token %}
    {{ form.as_p }}
    <button type="submit" class="btn btn-success">注册</button>
  </form>
  <p>
    已有账号？ <a href="{% url 'login' %}">登录</a>
  </p>
{% endblock %}
<!-- historical_chat/templates/registration/login.html -->

{% extends "base.html" %}

{% block content %}
  <h2>登录</h2>
  <form method="post">
    {% csrf_token %}
    {{ form.as_p }}
    <button type="submit" class="btn btn-primary">登录</button>
  </form>
  <p>
    还没有账号？ <a href="{% url 'chat:register' %}">注册</a>
  </p>
{% endblock %}
<!-- historical_chat/templates/registration/logout.html -->

{% extends "base.html" %}

{% block content %}
  <h2>已登出</h2>
  <p>您已成功登出。</p>
  <p><a href="{% url 'chat:index' %}">返回主页</a></p>
{% endblock %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>

</body>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>

</body>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>

</body>
</html><!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Title</title>
</head>
<body>

</body>
</html>from django.test import TestCase

# Create your tests here.
from django.urls import path
from . import views

app_name = 'chat'

urlpatterns = [
    path('ask/', views.ask_question, name='ask_question'),
    path('clear_memory/', views.clear_memory, name='clear_memory'),
    path('get_answer/', views.get_answer, name='get_answer'),
    path('register/', views.register, name='register'),  # 添加注册视图的 URL
    path('ask_difficult_question/', views.ask_difficult_question, name='ask_difficult_question'),
]
# utils.py 或适当的模块中

from src.Agents.API_info import ChatActionRunner
import logging
from src.Agents.API_info import ChatActionRunner
from src.Agents.start_Q import QuestionProcessor
from src.Agents.memory import config_w2v


# 初始化配置
config_w2v.initialize_calculator()
logger = logging.getLogger('chat')

class ChatService:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.processor = QuestionProcessor()

    async def get_response(self, user_question: str) -> str:
        try:
            response = await self.runner.execute(user_question)
            return response
        except RuntimeError as e:
            logger.error(f"获取AI回应时出错: {e}")
            return "抱歉，我无法处理您的请求。"


    async def handle_question(self, question: str, line_number=None):
        """
        Process the given question and return the results.

        :param question: The question to process.
        :param line_number: Optional, line number for processing context.
        :return: A dictionary containing plan_answer, answers, and final_answer.
        """
        try:
            # 调用 QuestionProcessor 处理问题
            plan_answer, answers, final_answer = await self.processor.process_question(question, line_number)

            # 构建结果字典
            result = plan_answer


            return result
        except Exception as e:
            # 捕获异常并打印错误信息
            print(f"An error occurred while processing the question: {e}")
            return None


# views.py
import json
from django.shortcuts import render, redirect
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.contrib.auth.forms import UserCreationForm
from django.contrib.auth import login
from django.contrib.auth.decorators import login_required
from .models import HistoricalFigure, Conversation, Message
import bleach
import logging
from .utils import ChatService
from asgiref.sync import async_to_sync

# 配置日志记录器
logger = logging.getLogger('chat')

# 实例化 ChatService
chat_service = ChatService()

def register(request):
    """
    处理用户注册
    """
    if request.method == 'POST':
        form = UserCreationForm(request.POST)
        if form.is_valid():
            user = form.save()
            login(request, user)  # 注册后自动登录
            logger.debug(f"新用户注册并登录: {user.username}")
            return redirect('chat:index')
    else:
        form = UserCreationForm()
    return render(request, 'chat/register.html', {'form': form})

@login_required
def index(request):
    """
    渲染聊天首页，展示前端页面
    """
    historical_figures = HistoricalFigure.objects.all()
    logger.debug(f"用户 {request.user.username} 访问聊天首页，历史人物数量: {historical_figures.count()}")
    return render(request, 'chat/index.html', {'historical_figures': historical_figures})

# views.py

@csrf_exempt
def ask_difficult_question(request):
    """
    接收问题并调用 Processor 中的异步函数进行处理
    """
    if request.method == "POST":
        try:
            # 获取前端传递的 JSON 数据
            data = json.loads(request.body)
            question = data.get('question', '')

            # 检查问题是否为空
            if not question:
                logger.error("未提供问题")
                return JsonResponse({
                    'status': 'error',
                    'message': '未提供问题'
                }, status=400)

            # 使用 async_to_sync 将异步函数转换为同步
            answer = async_to_sync(chat_service.handle_question)(question)

            # 返回问题的答案
            return JsonResponse({
                'status': 'success',
                'response': answer
            }, status=200)

        except json.JSONDecodeError:
            logger.error("请求数据不是有效的 JSON 格式")
            return JsonResponse({
                'status': 'error',
                'message': '请求数据不是有效的 JSON 格式'
            }, status=400)

        except Exception as e:
            logger.error(f"处理问题时发生错误: {str(e)}")
            return JsonResponse({
                'status': 'error',
                'message': str(e)
            }, status=500)

    else:
        logger.error("无效的请求方法")
        return JsonResponse({
            'status': 'error',
            'message': '无效的请求方法'
        }, status=405)
@csrf_exempt
@login_required
@csrf_exempt
@login_required
def ask_question(request):
    """
    处理用户提问历史人物的问题
    """
    if request.method == "POST":
        user_question = bleach.clean(request.POST.get('question', '').strip())
        figure_id = request.POST.get('figure_id')

        logger.debug(f"用户 {request.user.username} 提问: {user_question}, 历史人物ID: {figure_id}")

        # 验证输入
        if not user_question:
            logger.warning("用户提交了空的问题")
            return JsonResponse({'status': 'fail', 'message': '问题不能为空'}, status=400)
        if not figure_id:
            logger.warning("用户未提交历史人物ID")
            return JsonResponse({'status': 'fail', 'message': '历史人物ID不能为空'}, status=400)

        try:
            # 找到对应的历史人物
            figure = HistoricalFigure.objects.get(id=figure_id)
            logger.debug(f"找到历史人物: {figure.name}")
        except HistoricalFigure.DoesNotExist:
            logger.error(f"历史人物ID {figure_id} 不存在")
            return JsonResponse({'status': 'fail', 'message': '历史人物不存在'}, status=404)

        # 获取或创建 Conversation 对象
        conversation, created = Conversation.objects.get_or_create(
            user=request.user,
            historical_figure=figure
        )
        if created:
            logger.debug("创建新的会话记录")

        # 将提问保存到数据库（Message）
        message = Message(
            conversation=conversation,
            sender='user',  # 发送者是用户
            content=user_question
        )
        message.save()
        logger.debug("保存用户提问到数据库")

        # 构建提示词：历史人物系统消息 + 会话记忆 + 用户问题
        try:
            # 获取会话记忆（之前的消息内容）
            previous_messages = Message.objects.filter(conversation=conversation).order_by('id')
            memory = "\n".join(
                f"{msg.sender}: {msg.content}" for msg in previous_messages
            )

            # 使用 `prompt` 字段作为历史人物的引导信息
            figure_prompt = figure.prompt

            # 构造提示词
            prompt = (
                f"{figure_prompt}\n\n以下是你与用户的对话：\n"
                f"{memory}\n"
                f"用户的问题是：{user_question}\n"
                f"请基于你的角色身份和上下文做出回答。"
            )
            logger.debug(f"生成的提示词：{prompt}")

            # 使用 ChatService 获取回应
            response = async_to_sync(chat_service.get_response)(prompt)
            logger.debug(f"获取到AI回应: {response}")
        except Exception as e:
            logger.error(f"获取AI回应时出错: {e}")
            return JsonResponse({'status': 'fail', 'message': '无法获取AI回应'}, status=500)

        # 保存 AI 回复到数据库（Message）
        response_message = Message(
            conversation=conversation,
            sender='assistant',  # 发送者是 AI
            content=response
        )
        response_message.save()
        logger.debug("保存AI回应到数据库")

        return JsonResponse({'status': 'success', 'response': response})

    logger.warning("收到非POST请求")
    return JsonResponse({'status': 'fail', 'message': 'Invalid request'}, status=400)


@csrf_exempt
@login_required
def handle_clear_memory(request):
    """
    清除历史人物的记忆（假设清除的是与用户相关的会话）
    """
    if request.method == "POST":
        figure_id = request.POST.get('figure_id')

        # 验证输入
        if not figure_id:
            logger.warning("用户未提交历史人物ID")
            return JsonResponse({'status': 'fail', 'message': '历史人物ID不能为空'}, status=400)

        try:
            figure = HistoricalFigure.objects.get(id=figure_id)
            # 清除历史人物相关的所有对话记录
            Message.objects.filter(conversation__user=request.user, conversation__historical_figure=figure).delete()

            logger.debug(f"用户 {request.user.username} 清除 {figure.name} 的记忆")

            return JsonResponse({'status': 'success', 'message': f'{figure.name} 的记忆已清除'})
        except HistoricalFigure.DoesNotExist:
            logger.error(f"历史人物ID {figure_id} 不存在")
            return JsonResponse({'status': 'fail', 'message': '历史人物不存在'}, status=404)
        except Exception as e:
            logger.error(f"清除记忆时出错: {e}")
            return JsonResponse({'status': 'fail', 'message': '清除记忆时发生错误'}, status=500)

    logger.warning("收到非POST请求")
    return JsonResponse({'status': 'fail', 'message': 'Invalid request'}, status=400)

@csrf_exempt
@login_required
def get_answer(request):
    """
    获取用户提问的 AI 回答
    """
    if request.method == "POST":
        user_question = bleach.clean(request.POST.get('question', '').strip())

        # 验证输入
        if not user_question:
            logger.warning("用户提交了空的问题")
            return JsonResponse({'status': 'fail', 'message': '问题不能为空'}, status=400)

        # 获取 AI 回答（使用 async_to_sync 调用异步函数）
        try:
            response = async_to_sync(chat_service.get_response)(user_question)  # 直接传递字符串
            logger.debug(f"获取到AI回应: {response}")
        except Exception as e:
            logger.error(f"获取AI回应时出错: {e}")
            return JsonResponse({'status': 'fail', 'message': '无法获取AI回应'}, status=500)

        return JsonResponse({'status': 'success', 'response': response})

    logger.warning("收到非POST请求")
    return JsonResponse({'status': 'fail', 'message': 'Invalid request'}, status=400)

@csrf_exempt
@login_required
def clear_memory(request):
    """
    清除与某个历史人物的所有对话记录
    """
    if request.method == 'POST':
        figure_id = request.POST.get('figure_id')
        if not figure_id:
            logger.warning("用户未提交历史人物ID")
            return JsonResponse({'status': 'fail', 'message': '历史人物ID不能为空'}, status=400)

        try:
            # 获取历史人物
            historical_figure = HistoricalFigure.objects.get(id=figure_id)
            # 删除与该历史人物相关的对话记录
            deleted, _ = Conversation.objects.filter(
                user=request.user,
                historical_figure=historical_figure
            ).delete()
            logger.debug(f"用户 {request.user.username} 清除 {historical_figure.name} 的记忆")

            return JsonResponse({
                'status': 'success',
                'message': '聊天记忆已清除。',
                'deleted_conversations': deleted
            })
        except HistoricalFigure.DoesNotExist:
            logger.error(f"历史人物ID {figure_id} 不存在")
            return JsonResponse({'status': 'fail', 'message': '指定的历史人物不存在'}, status=404)
        except Exception as e:
            logger.error(f"清除记忆时出错: {e}")
            return JsonResponse({'status': 'fail', 'message': '清除记忆时发生错误'}, status=500)

    logger.warning("收到非POST请求")
    return JsonResponse({'status': 'fail', 'message': 'Invalid request'}, status=400)
# historical_chat/asgi.py

import os
from django.core.asgi import get_asgi_application

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'historical_chat.settings')

application = get_asgi_application()
import os
from celery import Celery

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'historical_chat.settings')

app = Celery('historical_chat')
app.config_from_object('django.conf:settings', namespace='CELERY')
app.autodiscover_tasks()
# historical_chat/settings.py

import os
from pathlib import Path
from dotenv import load_dotenv

# 加载 .env 文件
BASE_DIR = Path(__file__).resolve().parent.parent
load_dotenv(os.path.join(BASE_DIR, '.env'))

# 添加 src 目录到 Python 路径
import sys
sys.path.append(os.path.join(BASE_DIR, 'src'))

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = os.getenv('SECRET_KEY', 'your-default-secret-key')

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True  # 开发环境设为True，生产环境需设为False

ALLOWED_HOSTS = []  # 根据需要设置

# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'chat',  # 添加 chat 应用
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'historical_chat.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [BASE_DIR / 'templates'],  # 包含全局模板目录
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',  # 必须包含
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'historical_chat.wsgi.application'
ASGI_APPLICATION = 'historical_chat.asgi.application'  # 确保 ASGI_APPLICATION 被设置

# Database
# https://docs.djangoproject.com/en/4.2/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': BASE_DIR / 'db.sqlite3',
    }
}

# Password validation
# https://docs.djangoproject.com/en/4.2/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]

# Internationalization
# https://docs.djangoproject.com/en/4.2/topics/i18n/

LANGUAGE_CODE = 'zh-hans'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True

# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/4.2/howto/static-files/

STATIC_URL = '/static/'
STATICFILES_DIRS = [
    BASE_DIR / "static",  # 如果有全局静态文件目录
]

# Default primary key field type
# https://docs.djangoproject.com/en/4.2/ref/settings/#default-auto-field

DEFAULT_AUTO_FIELD = 'django.db.models.BigAutoField'

# Logging configuration
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
        },
    },
    'loggers': {
        # 调整 django.template 的日志级别
        'django.template': {
            'handlers': ['console'],
            'level': 'ERROR',  # 只记录 WARNING 级别及以上的日志
            'propagate': False,
        },
        # 其他日志配置
        'chat': {
            'handlers': ['console'],
            'level': 'DEBUG',
        },
    },
}


# CSRF and Authentication settings
LOGIN_REDIRECT_URL = '/chat/'
LOGIN_URL = '/accounts/login/'  # 默认登录 URL
LOGOUT_REDIRECT_URL = '/accounts/login/'
# historical_chat/urls.py

from django.contrib import admin
from django.urls import path, include
from chat import views
urlpatterns = [
    path('admin/', admin.site.urls),
    path('chat/', include('chat.urls', namespace='chat')),  # 包含 chat 应用的 URL
    path('accounts/', include('django.contrib.auth.urls')),   # 添加认证 URL
path('', views.index, name='home'),  # 为空路径设置视图
]
"""
WSGI config for historical_chat project.

It exposes the WSGI callable as a module-level variable named ``application``.

"""

import os
from django.core.wsgi import get_wsgi_application

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'historical_chat.settings')

application = get_wsgi_application()
from .celery import app as celery_app

__all__ = ('celery_app',)
#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys

def main():
    """Run administrative tasks."""
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'historical_chat.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)

if __name__ == '__main__':
    main()
import json
import random


def filter_and_sample_data(file_path, sample_size_per_category):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)

        # 筛选出答案为 'yes' 和 'no' 的数据
        yes_answers = [item for item in data if item['answer'].lower() == 'yes']
        no_answers = [item for item in data if item['answer'].lower() == 'no']

        # 确保每个类别的数量足够进行抽样
        if len(yes_answers) < sample_size_per_category or len(no_answers) < sample_size_per_category:
            raise ValueError("Not enough data to sample from one or both categories.")

        # 从每个答案类别中随机抽取指定数量的数据
        sampled_yes = random.sample(yes_answers, sample_size_per_category)
        sampled_no = random.sample(no_answers, sample_size_per_category)

        return sampled_yes + sampled_no


def save_sampled_data(sampled_data, output_file):
    with open(output_file, 'w', encoding='utf-8') as file:
        json.dump(sampled_data, file, ensure_ascii=False, indent=4)  # 保存到 JSON 文件，格式化输出


# 主执行流程
file_path = '/home/yang/home/yang/workspace/code/xagent/data/hotpot_dev_distractor_v1.json'
output_file = '/home/yang/home/yang/workspace/code/xagent/data/sample_100_data.json'
sample_size_per_category = 75

sampled_data = filter_and_sample_data(file_path, sample_size_per_category)

# 转换数据结构，只保留需要的字段
filtered_data = []
for item in sampled_data:
    result = {
        "id": item['_id'],
        "Question": item['question'],
        "Answer": item['answer'],
        "Difficulty Level": item['level'],
        "All Context Texts": []
    }
    for context in item['context']:
        # 假设每个 context 项是一个元组(title, texts)
        result["All Context Texts"].extend(context[1])  # 将每个文本块加入列表
    filtered_data.append(result)

# 保存筛选和处理后的数据
save_sampled_data(filtered_data, output_file)
import json

# Paths to the JSON files
questions_path = '/home/yang/home/yang/workspace/code/xagent/data/sampled_questions.json'
emails_path = '/home/yang/home/yang/workspace/code/xagent/data/emails.json'

# Function to load data from a JSON file
def load_data_from_json(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

# Load data
questions_data = load_data_from_json(questions_path)
emails_data = load_data_from_json(emails_path)

# Function to insert questions into the email bodies
def insert_questions(emails, questions):
    question_index = 0
    for email in emails:
        # Ensure there are enough questions to insert
        if question_index + 5 > len(questions):
            break
        inserted_questions = "\n".join(q["question"] for q in questions[question_index:question_index+5])
        email["body"] = email["body"].replace("[ ]", inserted_questions)
        question_index += 5
    return emails

# Process the emails
updated_emails = insert_questions(emails_data, questions_data)

# Save the updated emails to a JSON file
output_path = '/home/yang/home/yang/workspace/code/xagent/data/updated_emails.json'
with open(output_path, 'w', encoding='utf-8') as file:
    json.dump(updated_emails, file, ensure_ascii=False, indent=4)
import json

# 加载 JSON 文件
with open('emails.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# 提取 question_ids 并移除原数据中的 question_ids
question_ids_list = []

for item in data:
    if 'question_ids' in item:
        question_ids_list.extend(item.pop('question_ids'))

# 将去除 question_ids 的数据保存到原文件
with open('emails.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

# 将提取出的 question_ids 保存到新文件中
with open('emails_ids.json', 'w', encoding='utf-8') as f:
    json.dump(question_ids_list, f, ensure_ascii=False, indent=4)

print("question_ids 已成功提取并保存。")
from transformers import BertTokenizer, BertModel
import os

def download_bert_model(model_name="bert-base-uncased", save_directory="./EV_NER"):
    # 创建保存目录（如果不存在）
    os.makedirs(save_directory, exist_ok=True)

    # 加载分词器
    tokenizer = BertTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(save_directory)  # 保存分词器到指定目录

    # 加载模型
    model = BertModel.from_pretrained(model_name)
    model.save_pretrained(save_directory)  # 保存模型到指定目录

if __name__ == "__main__":
    download_bert_model(save_directory="/home/zhaorh/code/ppagent/model")
import transformers
import torch

# 设置模型ID和本地保存路径
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
save_directory = "./my_llama_model"

# 下载并保存模型到指定目录
model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    cache_dir=save_directory
)

tokenizer = transformers.AutoTokenizer.from_pretrained(
    model_id,
    cache_dir=save_directory
)

# 创建推理pipeline
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
)

# 测试模型输出
messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

outputs = pipeline(messages, max_new_tokens=256)

# 打印结果
print(outputs[0]["generated_text"])
import os
from gensim.models import KeyedVectors
from gensim.downloader import base_dir


def load_data():
    path = os.path.join(base_dir, 'word2vec-google-news-300', "word2vec-google-news-300.gz")
    model = KeyedVectors.load_word2vec_format(path, binary=True)
    return model
import asyncio
from src.Agents.actions.action_output import chat_completion_to_dict
class Action:
    def __init__(self, chat_client):
        self.chat_client = chat_client


    async def run(self, user_input, model):
        loop = asyncio.get_running_loop()
        # 使用 run_in_executor 允许在异步函数中调用同步方法
        response = await loop.run_in_executor(None, self.chat_client.fetch_chat_response, user_input, model)
        return chat_completion_to_dict(response)




def chat_completion_to_dict(completion):
    # 将每个部分的内容转换为字典格式
    return {
        'id': completion.id,
        'choices': [
            {
                'finish_reason': choice.finish_reason,
                'index': choice.index,
                'logprobs': choice.logprobs,
                'message': {
                    'content': choice.message.content,
                    'role': choice.message.role,
                    'function_call': choice.message.function_call,
                    'tool_calls': choice.message.tool_calls
                }
            } for choice in completion.choices
        ],
        'created': completion.created,
        'model': completion.model,
        'object': completion.object,
        'service_tier': getattr(completion, 'service_tier', 'default_tier'),
        'system_fingerprint': completion.system_fingerprint,
        'usage': {
            'completion_tokens': completion.usage.completion_tokens,
            'prompt_tokens': completion.usage.prompt_tokens,
            'total_tokens': completion.usage.total_tokens
        }
    }
import asyncio
from src.Agents.actions.action import Action
from src.LLM_Model.GPT_api_request import ChatClient

class ChatActionRunner:
    def __init__(self, model="gpt-4o"):
        self._api_key = 'sk-nTaeIVSY4P3w9dnLXQa5ar1xIKhE04dL1D6pXmG4JUabvnnG'  # API key is now a fixed, private attribute
        self._model = model
        self._chat_client = ChatClient(self._api_key)
        self._action = Action(self._chat_client)

    async def _run_action(self, content):
        try:
            response = await self._action.run(content, self._model)
            # Extract the final answer here
            answer = response['choices'][0]['message']['content']
            return answer
        except Exception as e:
            # Log the exception, raise an error, or handle it as needed
            raise RuntimeError(f"An error occurred during the API request: {str(e)}")

    async def execute(self, content):
        # Directly await the private method without using asyncio.run
        try:
            return await self._run_action(content)
        except RuntimeError as e:
            # Handle the error or rethrow it after logging
            print(e)
            return {"error": str(e)}


'''import aiohttp
import asyncio

class ChatActionRunner:
    def __init__(self, model="llama3.1:70b"):
        self._model = model
        self._url = 'http://172.18.144.13:11434/api/chat'

    async def _run_action(self, content):
        try:
            data = {
                "model": self._model,
                "messages": [{ "role": "user", "content": content }],
                "stream": False
            }
            async with aiohttp.ClientSession() as session:
                async with session.post(self._url, json=data) as response:
                    response.raise_for_status()  # Raise exception for HTTP errors
                    result = await response.json()
                    # Return only the content of the assistant's message
                    return result.get('message', {}).get('content', 'No content in response')
        except Exception as e:
            # Log the exception, raise an error, or handle it as needed
            raise RuntimeError(f"An error occurred during the API request: {str(e)}")

    async def execute(self, content):
        # Directly await the private method without using asyncio.run
        try:
            return await self._run_action(content)
        except RuntimeError as e:
            # Handle the error or rethrow it after logging
            print(e)
            return {"error": str(e)}
'''


from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio



class CheckMan:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.repairman = '''
        You are an expert in Entertainment with an Extremely High level of expertise.
        Answer this question：question 5: What was Eddie Murphy's first movie?
        The answer should be as comprehensive as possible and should not exceed 20 words.Check before replying.
        Use formal language.
        Do not use abbreviations when replying to movie names, people names, or place names.'''

    async def repair(self, question,all_answer):
        try:
            print("------------------------------------repair_person----------------------------------------")

            # 并行执行多个异步操作
            finally_answer = await self.runner.execute(self.repairman)
            print("------------------------------------repair_end----------------------------------------")
            return finally_answer
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

check_man = CheckMan()

# 异步执行 repair 方法
async def main():
    question = "aswdadas"
    all_answer = None  # 如果你有其他答案，可以传入
    result = await check_man.repair(question, all_answer)
    print(f"Final answer: {result}")

# 运行异步任务
asyncio.run(main())import asyncio  # Import asyncio for running asynchronous code

from src.Agents.roles.sub_q_check import SubCheckMan


async def main():
    a_question = 'Who was the youngest brother in the Beach Boys?'
    max_iterations = 4  # Maximum loop iterations
    iterations = 0  # Counter initialization
    check_result_man = SubCheckMan()  # Instantiate the SubCheckMan class
    contradictions = True  # Initialize contradictions flag
    result = 'None'  # Starting result

    while contradictions and iterations < max_iterations:
        # Check if the result is correct using the check method
        check_result = await check_result_man.check(a_question, result)

        if isinstance(check_result, tuple) and len(check_result) == 2:
            # If the check returns a tuple, evaluate the result
            contradictions = not check_result[0]  # Set contradictions based on check_result[0]
            result = check_result[1]  # Update the result
        elif isinstance(check_result, bool):
            # If the check returns a boolean, set contradictions based on its value
            contradictions = not check_result  # Set contradictions based on check_result
            if contradictions:
                result = "No answer found"  # Set default result if no valid answer is found
        else:
            # Handle unexpected result format
            contradictions = True
            result = "Unexpected result format"

        # Update iterations and print current state
        iterations += 1
        print(f"Contradictions: {contradictions}")
        print(f"Result: {result}")


# Run the async main function using asyncio
asyncio.run(main())
from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio



class Answer:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.question_format_0 = '''Answer this question：{Question}，
        The answer should be as comprehensive as possible and should not exceed 20 words.
        Please read my question carefully, grasp every word accurately, and answer after sufficient reasoning.
        If it is a binary question, answer true or false.
        If it is a question about the timeline, please search your knowledge base based on the question and think carefully before answering.
        '''

        self.question_format_1 = '''Answer this question：{Question}，
                You can refer to the answers to these:{some_answers}
                The answer should be as comprehensive as possible and should not exceed 20 words.
                Please read my question carefully, grasp every word accurately, and answer after sufficient reasoning.
                If it is a binary question, answer true or false.
                If it is a question about the timeline, please search your knowledge base based on the question and think carefully before answering.
                '''

    async def answer_sub(self, question,prompt,some_answers):
        try:

            if not some_answers:  # 如果 some_answers 为空
                # 执行 A 操作（按你原来的逻辑构建 all_content）
                all_content = self.question_format_0.format(Question=question)
                all_content = prompt  + all_content
                print('prompt for sub_field_question', all_content)
                print("------------------------------------Sub-questions----------------------------------------")
                # 并行执行多个异步操作
                finally_answer = await self.runner.execute(all_content)


            else:  # 如果 some_answers 不为空，执行 B 操作
                # 假设 B 操作是修改 all_content 或其他操作
                all_content = self.question_format_1.format(Question=question,some_answers =some_answers)
                all_content = prompt + all_content
                print('sub question', all_content)
                print("------------------------------------Sub-questions----------------------------------------")

                # 并行执行多个异步操作
                finally_answer = await self.runner.execute(all_content)

            print("------------------------------------end----------------------------------------")
            return finally_answer
        except Exception as e:
            print(f"An error occurred: {e}")
            return None


"""import asyncio

# Assuming the Answer class is imported or defined in your script

# Create an instance of the Answer class
answer_instance = Answer()

# Define the question and prompt you want to pass
question = "How many runs did Donald Bradman score in his last ever test match innings?"
prompt = "you are a expert. "

# Use asyncio to run the answer_sub method, since it's an async function
async def get_answer():
    result = await answer_instance.answer_sub(question, prompt)
    print(f"Answer: {result}")

# Run the async function
asyncio.run(get_answer())"""
import asyncio
from src.Agents.roles.collector import QuestionCollector
from src.Agents.roles.expert_doing import ExpertCooperation
from src.Agents.roles.select_experter import ExpertSelector, IndustryExtractor
from src.text_analysis.word2vec_computer import SimilarityMatrixCalculator
from src.Agents.TF_expert import DomainScoreCalculator
from src.text_analysis.Google_text_analysis import  SimilarityCalculator
from src.Agents.memory import config_w2v
import numpy as np
def create_expert_sentence(expertise_areas):
    if not expertise_areas:
        return "You're an expert."

    # 构建基本句子
    expertise_sentence = "You are an expert in"

    # 遍历列表，添加每个领域的专家信息
    for i, (area, level) in enumerate(expertise_areas):
        if i == len(expertise_areas) - 1:  # 如果是最后一个元素，不添加逗号
            expertise_sentence += f" {area} with an {level} level of expertise."
        else:
            expertise_sentence += f" {area} with an {level} level of expertise, "

    return expertise_sentence





def find_max_domain(matrix, domains):
    max_domains = []

    for row in matrix:
        if np.any(row):  # 如果该行存在非零元素
            max_index = np.argmax(row)  # 找到每一行中最大值的位置
            max_domain = domains[max_index]  # 找到对应的领域
        else:
            max_domain = "none"  # 如果全为零，返回 'none'

        max_domains.append(max_domain)

    return max_domains

class ExpertPick:
    def __init__(self):
        pass



    async def process(self, question):
        try:
            # 收集问题相关信息
            collector = QuestionCollector()
            collected_information = await collector.collect(question)
            print(f"Collected Information: {collected_information}")

            # 增强问题
            enhanced_question = f"{question}: {collected_information}"
            print(f"Enhanced Question: {enhanced_question}")

            # 选择专家
            experts = ExpertSelector()
            expert_information = await experts.elector_expert(enhanced_question)
            print(f"Expert Information: {expert_information}")

            # 提取领域信息
            extractor = IndustryExtractor(expert_information)
            result = extractor.extract()
            print(f"Extracted Result: {result}")
            if  result:
                calculator_TF = DomainScoreCalculator()

                # 打印领域归属分数
                domain_scores = await calculator_TF.get_domain_scores(question)
                for domain, score in domain_scores.items():
                    print(f"Domain: {domain}, Normalized Score: {score:.4f}")
                scores = list(domain_scores.values())

                # 假设config_w2v已经有一个calculator实例，带有calculate_similarity方法

                matrix_calculator = SimilarityMatrixCalculator(result, config_w2v.calculator)

                # 计算并打印相似度矩阵
                matrix_calculator.calculate_matrix()
                similarity_matrix=matrix_calculator.return_matrix()
                similarity_matrix = np.nan_to_num(similarity_matrix, nan=0)
                print(similarity_matrix)
                TF_matrix = np.broadcast_to(scores, similarity_matrix.shape)
                TF_w2v_matrix =(0.95*similarity_matrix) +(0.05*TF_matrix)


                domains = ["Entertainment", "financial", "History", "legal", "Literature",
                           "medical", "Politics", "Sports", "technology", "science"]

                # 找出每一行中最大值对应的领域
                expert_field = find_max_domain(TF_w2v_matrix, domains)
                print(expert_field)
                new_result = [(expert_field[i], description) for i, (_, description) in enumerate(result)]
                print('new',new_result)
                filtered_data = [item for item in new_result if item[0] != 'none']
                filtered_data = [item for item in filtered_data if item[0].lower() != 'none' and 'low' not in item[1].lower()]
                merged_data = {}
                for item in filtered_data:
                    domain, score = item
                    if domain not in merged_data:
                        merged_data[domain] = score

                # 转换为最终结果列表
                final_data = list(merged_data.items())

                sentence = create_expert_sentence(final_data)
            else:
                sentence = "You're an expert."
            return sentence, final_data
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

import json
import re

class FinalAnswerExtractor:
    def __init__(self, file_path):
        self.file_path = file_path
        self.contents = self.extract_contents_from_json()
        self.final_answers = self.extract_final_answers()

    def extract_contents_from_json(self):
        contents = []
        with open(self.file_path, 'r', encoding='utf-8') as file:
            file_content = file.read()
            file_content = re.sub(r'}\s*{', '},{', file_content)
            file_content = '[' + file_content + ']'
            try:
                data = json.loads(file_content)
            except json.JSONDecodeError as e:
                print(f"Failed to decode JSON: {e}")
                return []

            def recurse_extract(data):
                if isinstance(data, dict):
                    for key, value in data.items():
                        if key == 'content':
                            contents.append(value)
                        else:
                            recurse_extract(value)
                elif isinstance(data, list):
                    for item in data:
                        recurse_extract(item)

            recurse_extract(data)
        return contents

    def extract_final_answers(self):
        final_answers = []
        pattern = r'Final answer:(.+)'
        for content in self.contents:
            matches = re.findall(pattern, content, re.DOTALL)
            if matches:
                final_answers.extend([m.strip() for m in matches])
            else:
                print("No 'Final answer' found in content:", content)
        return final_answers

    def get_answer_by_index(self, index):
        if index - 1 < len(self.final_answers):
            return self.final_answers[index - 1]
        else:
            return "No answer available for this index"

# Usage


from src.text_analysis.Google_text_analysis import SimilarityCalculator

calculator = None  # 全局变量

def initialize_calculator():
    """初始化 calculator 变量"""
    global calculator
    if calculator is None:
        calculator = SimilarityCalculator()
        print(" Word2vec Calculator initialized.")
    else:
        print("Calculator already initialized.")from src.Agents.API_info import ChatActionRunner
from src.Agents.memory.key_memory import *
import asyncio



class DisassemblyProblem:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.Disassembly = Disassembly

    async def disassembly(self, questions):
        try:

            print("------------------------------------Disassembly---------------------------------------")
            all_content = self.Disassembly.format(Questions=questions)
            # 并行执行多个异步操作
            finally_answer = await self.runner.execute(all_content)
            print(finally_answer)

            print("------------------------------------Disassembly_end----------------------------------------")

            return finally_answer
        except Exception as e:
            print(f"An error occurred: {e}")
            return NoneDisassembly = '''
I need to complete a task like {Questions}.
I need you to extract the sub-questions in this task and send them to me in this form: //question 1://, //question 2://, //question 3://, and so on.If the question is A, The format should be like this //question 1: A//
'''from src.LLM_Model.Llama_8B import TextGenerator


# 全局变量缓存 TextGenerator 实例
generator_instance = None

def get_generator(model_path="/home/zhaorh/code/ppagent/model/Llama-7B"):
    """获取 TextGenerator 实例。如果未初始化则初始化。"""
    global generator_instance
    if generator_instance is None:
        generator_instance = TextGenerator(model_path)
        print("TextGenerator initialized.")
    else:
        print("TextGenerator already initialized.")
    return generator_instancefrom src.Preprocess_data.Desensitization import EntityEncryptor

# 全局变量缓存 EntityEncryptor 实例
encryptor_instance = None

def get_encryptor():
    """获取 EntityEncryptor 实例。如果未初始化则初始化。"""
    global encryptor_instance
    if encryptor_instance is None:
        encryptor_instance = EntityEncryptor(
            model_path="/home/zhaorh/code/ppagent/model/NER",
            tokenizer_path="/home/zhaorh/code/ppagent/model/NER",
            device=1
        )
        print("EntityEncryptor initialized.")
    else:
        print("EntityEncryptor already initialized.")
    return encryptor_instance


import asyncio
from src.Agents.roles.collector import QuestionCollector
from src.text_analysis.TF_IDF import TextDomainAnalyzer
from src.Agents.roles.expert_doing import ExpertCooperation
from src.Agents.roles.select_experter import ExpertSelector, IndustryExtractor
from src.Agents.roles.Answer_Fusion import AnswerFusion
from src.Agents.roles.Repairman import RepairPerson
from src.Agents.memory.Disassembly_Problem import DisassemblyProblem
from src.Agents.roles.sub_q_check import SubCheckMan
import re
from src.Agents.expert import ExpertPick
from src.Agents.demo2 import Answer
# 异步主函数
class Planner:
    def __init__(self, question):
        self.fields = None
        self.question = question

    def create_expert_sentences(self):
        # 用来存储生成的句子
        sentences = []

        # 遍历 fields 列表并生成句子
        for field, level in self.fields:
            sentence = f"You are an expert in {field} with an {level} level of expertise."
            sentences.append(sentence)

        return sentences

    async def run(self):
        dis_question =DisassemblyProblem()
        questions_sub = await dis_question.disassembly(questions=self.question)
        print(questions_sub)
        results = []
        a_questions = re.findall(r'//\s*(.*?)\s*//', questions_sub)
        print(a_questions)
        for i, a_question in enumerate(a_questions, 1):
            #领域专家
            a = Answer()
            processor = ExpertPick()
            expert_prompt,self.fields = await processor.process(a_question)
            sentences = self.create_expert_sentences()
            field_results = []
            for sentence in sentences:
                print(sentence)
                field_result = await a.answer_sub(a_question,sentence,"")
                print(field_result)
                field_results.append(field_result)
            print(field_results)
            final_sentence = " ".join(field_results)
            result = await a.answer_sub(a_question,expert_prompt,final_sentence)
            print(result)
            if result is None:
                expert_prompt ='you are a expert,'
                result = await a.answer_sub(a_question, expert_prompt,final_sentence)
            b = result
            print('sub question result:',result)
            max_iterations = 4  # 设置最大循环次数
            iterations = 0  # 初始化计数器
            check_result_man = SubCheckMan()
            contradictions = True
            while contradictions and iterations < max_iterations:
                # Check if the result is correct using the check method
                check_result = await check_result_man.check(a_question, result)

                if isinstance(check_result, tuple) and len(check_result) == 2:
                    # If the check returns a tuple, evaluate the result
                    contradictions = not check_result[0]  # Set contradictions based on check_result[0]
                    result = check_result[1]  # Update the result
                elif isinstance(check_result, bool):
                    # If the check returns a boolean, set contradictions based on its value
                    contradictions = not check_result  # Set contradictions based on check_result
                else:
                    # Handle unexpected result format
                    contradictions = True

                # Update iterations and print current state
                iterations += 1
            if result =='No answer found':
                result = b
            results.append(result)
        answer_fusion = AnswerFusion()
        fusion_answer = await answer_fusion.fusion(self.question,results)
        finally_answer = RepairPerson()
        final_answer = await finally_answer.repair(self.question, results,fusion_answer)

        return fusion_answer,results,final_answer




import re
import asyncio
from src.Agents.memory.NER import get_encryptor
from src.Agents.memory.llama_8B import TextGenerator

def perform_action_A(key, value):
    # 定义 A 操作的逻辑
    print(f"A操作: '{key}' 不被认为是敏感信息。")

def perform_action_B(question, key, value):
    # B操作：将问题中的 key 替换为 value
    new_question = re.sub(rf'\b{re.escape(key)}\b', value, question)
    print(f"B操作: 将 '{key}' 替换为 '{value}'。")
    return new_question

def extract_privacy_data(text):
    """提取文本中的隐私数据"""
    patterns = {
        'email': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
        'phone': r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
        'ip_address': r'(\d{1,3}\.){3}\d{1,3}',
        'credit_card': r'\b(?:\d{4}[-.\s]?){3}\d{4}\b',
        'location': r'\b\d+\.\d+\s*,\s*\d+\.\d+\b',
        'id_number': r'\b\d{6}-\d{4}\b',
        'social_security': r'\b\d{3}-\d{2}-\d{4}\b',
        'medical_record': r'\bMRN:\s*\d+\b',
    }

    extracted_data = {}
    for key, pattern in patterns.items():
        matches = re.findall(pattern, text)
        if matches:
            extracted_data[key] = matches

    return extracted_data

def mask_partial(data, category):
    """根据类别进行部分掩码处理"""
    if category == 'email':
        return re.sub(r'(\w{1})[\w.-]+(@[\w.-]+)', r'\1****\2', data)
    elif category == 'phone':
        return re.sub(r'(\(?\d{3}\)?[-.\s]?)\d{3}([-.\s]?\d{4})', r'\1***\2', data)
    elif category == 'ip_address':
        return re.sub(r'(\d{1,3}\.\d{1,3}\.)\d{1,3}', r'\1***', data)
    elif category == 'credit_card':
        return re.sub(r'(\d{4}[-.\s]?){3}(\d{4})', r'****-****-****-\2', data)
    elif category in ['id_number', 'social_security']:
        return re.sub(r'(\d{3}-\d{2})-\d{4}', r'\1-****', data)
    elif category == 'medical_record':
        return re.sub(r'(MRN:\s*)\d+', r'\1****', data)
    else:
        return data  # 如果不匹配，不改变数据

def mask_and_replace(text, privacy_data, generator):
    """根据模型判断是否掩码并替换文本中的隐私数据"""
    masked_text = text  # 初始为原始文本

    for category, items in privacy_data.items():
        if items:  # 仅当有匹配项时处理
            for item in items:
                message = (
                    f"Determine if the following data '{item}' belongs to the category '{category}': "
                    "Please respond with 'Yes' or 'No'."
                )
                messages = [{"role": "user", "content": message}]
                response = generator.generate_text(messages, '')

                print(f"Response for '{item}': {response}")

                if "yes" in response.lower():
                    # 使用部分掩码替换
                    masked_item = mask_partial(item, category)
                    masked_text = masked_text.replace(item, masked_item)

    print("Encrypted new_question:", masked_text)  # 打印加密后的文本
    return masked_text


async def privacy_part(question):
    # 初始化加密器和生成器
    encryptor = get_encryptor()
    encrypted_text, replacement_dict = encryptor.encrypt_entities(question)
    # 去除相邻重复的隐私名称
    pattern = r'(<[^>]+>)(?:\s*<[^>]+>)+'
    encrypted_text = re.sub(pattern, r'\1', encrypted_text)

    print("Encrypted text:", encrypted_text)
    print("Entity replacement dictionary:", replacement_dict)
    new_question = question
    # 初始化文本生成器（同步调用）
    generator = TextGenerator(model_path="/home/zhaorh/code/ppagent/model/Llama-7B")  # 请将路径替换为实际模型路径

    # 遍历替换字典，逐项询问是否会影响任务解答
    for key, value in replacement_dict.items():
        message = (
            """Determine if the word "{key}" represents private data relevant to the task: {task}. 

### Private data may include:
 
1. **Personal names** related to individuals (such as user, recipient, or sender).
2. **Addresses or locations** tied to the user, recipient, or sender.
3. **Organizations or affiliations** connected with the user, recipient, or sender.

Consider whether the word "{key}" could be classified as personal or organizational information directly associated with someone involved in the context. 

If it aligns with the above categories, respond with **"Yes."** If it does not, respond with **"No."**
"""
        )

        # 填充 key 和 task

        message = message.format(key=key, task=question)
        # 将 message 包装为符合格式的列表
        messages = [{"role": "user", "content": message}]

        # 同步调用 generate_text，不使用 await
        response = generator.generate_text(messages, '')
        number_message = ("""
        """)
        print(f"Response for '{key}': {response}")
        if "yes" in response.lower():
            new_question = perform_action_B(new_question, key, value)  # B操作
        else:
            perform_action_A(key, value)  # A操作
    print(new_question)
    privacy_data = extract_privacy_data(new_question)
    # 输出处理后的隐私数据
    masked_new_question = mask_and_replace(new_question, privacy_data, generator)
    print(masked_new_question)


    return encrypted_text, replacement_dict,masked_new_question


from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio

class AnswerFusion:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.expert_fusion = Columbus.Answer_Fusion_Prompt

    async def fusion(self, task,results):
        try:
            content = ""
            number = 1
            for result in results:
                content = content + str(number) + ':' + result
                number += 1
            all_answer= self.expert_fusion.format(task=task,content=content)
            print("------------------------------------Answer_Fusion---------------------------------------------")

            # Execute the query asynchronously
            final_answer = await self.runner.execute(all_answer)
            print("------------------------------------Fusion_end------------------------------------------------")

            # Extract and return the relevant message from the response

            return final_answer
        except Exception as e:
            print(f"An error occurred: {e}")
            return Nonefrom src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio



class CheckMan:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.repairman = Columbus.repairman

    async def repair(self, question,all_answer):
        try:
            all_content = self.repairman.format(Question=question,answer=all_answer)
            print("------------------------------------repair_person----------------------------------------")

            # 并行执行多个异步操作
            finally_answer = await self.runner.execute(all_content)
            print("------------------------------------repair_end----------------------------------------")
            return finally_answer
        except Exception as e:
            print(f"An error occurred: {e}")
            return None


from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio

class QuestionCollector:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.Segmentation = Columbus.Segmentation

    async def collect(self, question):
        try:
            all_content = self.Segmentation.format(question=question)
            print("------------------------------------collect_information----------------------------------------")

            # 并行执行多个异步操作
            related_message =await self.runner.execute(all_content)
            print("------------------------------------collect_end----------------------------------------")
            return related_message
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

experts ='''
You are an expert in the field of {field},The degree of affiliation is{score}

Please collect relevant information as comprehensively as possible. 

please analyze the following questions

question:{question}
'''
analyzy = '''
As an expert skilled in classifying fields and proficient in using encyclopedias, your task is to analyze key information within provided questions, identify relevant names, places, and related events, and categorize this information into its appropriate field.

Question and related_information: {collect_information}

Identify and Collect Key Information: Please identify the key individuals, geographical locations, and any significant related events mentioned in the question or implicated by the answer.

Classify the Field:

Indicate the field that this information most closely relates to.

Score the Degree of Relevance to the Field:

Evaluate and assign a relevance score to how closely the information pertains to the identified field, using the following scale:
Relevance Score:

Extremely Low

Low

Moderately Low

Medium

Moderately High

High

Extremely High

Please provide a comprehensive assessment based on the above guidelines, ensuring that your evaluation reflects both the precision of the information’s classification and the depth of its relevance to the specified field.
You don't need to answer other information, just the field and score.I'll give you an example.You must put the Field you belong to between the two **.
Try to use one word for the field
//Reply Method://

1. **Field 1**
Relevance Score:Degree of affiliation

2. **Field 2**
Relevance Score:Degree of affiliation

3. **Field 3**
Relevance Score:Degree of affiliation


'''

Segmentation = '''
The question is: {question}

Key points to address:

1. Provide a detailed explanation or background information on the main topic of the question.

2. Identify and describe any significant people, places, events, or concepts related to the question.

3. Discuss the historical context, origin, and development of the main topic.

4. Analyze the impact, significance, and relevance of the topic in contemporary society or specific fields.

5. Highlight any controversies, debates, or differing perspectives associated with the question.

6. Provide examples, anecdotes, or case studies to illustrate key points.

Ensure the information is comprehensive and concise, covering all aspects of the question within 40 tokens.
'''

Answer_Fusion_Prompt = '''
I need to complete a task :{task}// 
through the reasoning of experts,
 I got the answers to some of the sub-questions :{content} helped us complete the task, 
 now you help me complete the task.The completeness of your answer must be more important than the fluency of your answer.
  Remember, you must carefully memorize the answers to the sub-questions I give you and incorporate them into your answer.
Try not to miss any words or information about the answers to the sub-questions.
If the sub-question is a binary question, the corresponding sub-answers also need to be incorporated, such as true and false.
'''

repairman = '''


You are a quality assurance expert.//
Here is a task and the answer to the task. //
Task: {Question}
Solutions to some of these sub-problems:{content}
final solution: {answer}
Attention!Your need to analyze the task and check the final solution. 
If you think the answer is perfect,  reply Format:
//the solution is perfect//.
If you think your solution does not cover all the answers to the sub-problems, 
please improve on the original solution and write an improved solution.

  
 
'''


sub_repairman ='''
You are an expert at fixing problems.
I had a question {question} and got a solution : {answer}, 
You have to use multiple knowledge bases to think.
do you think this is right? 
if you think this is right.
You should reply:
//you are correct//.
if you don't think it's right .
You should reply:
 //you are wrong//, correct answer：// Write your correct answer here//
'''



from src.LLM_Model.Llama_8B import TextGenerator


class EmailEntityExtractor:
    def __init__(self, model_path="/home/zhaorh/code/ppagent/model/Llama-7B"):
        self.generator = TextGenerator(model_path)

        self.messages_name = [{"role": "user",
                               "content": "Extract the names of people mentioned in the following document. If there are no names, respond with 'No names found.'"}]
        self.messages_location = [{"role": "user",
                                   "content": "Extract the place names mentioned in the following document. If there are no place names, respond with 'No place names found.'"}]
        self.messages_organization = [{"role": "user",
                                       "content": "Extract the organization names mentioned in the following document. If there are no organization names, respond with 'No organization names found.'"}]
        self.message_mark_privacy = [{"role": "user",
                                      "content": "Extract the private data of the letter, such as the name of the writer, the recipient, where it comes from, and the organization"}]

    def extract_entities(self, doc):
        response_name = self.generator.generate_text(self.messages_name, doc)
        print('name', response_name)
        response_location = self.generator.generate_text(self.messages_location, doc)
        print('location', response_location)
        response_organization = self.generator.generate_text(self.messages_organization, doc)
        print('organization', response_organization)
        return response_name, response_location, response_organization

    def mark_privacy(self, letter):
        response_privacy = self.generator.generate_text(self.message_mark_privacy, letter)
        return response_privacy


# 使用示例
extractor = EmailEntityExtractor()

document = """Were Thinking Fellers Union Local 282 and the Smiths, who had a lead singer of Morrissey, both active in 1986?"""
response_name, response_location, response_organization = extractor.extract_entities(document)

# 示例信件
letter = """
Dear Emma Williams,
I am from Denver, and I am a part of the Rocky Mountain Historical Society.
 I would like to ask you a few questions.
  John Coffin Talbot served as a legislator in the Maine Legislature, Where does this state's legislature convene?
  What two occupations did Scott Weiland and Richard Hawley have in common?
  Thomas Maclellan of Bombie was Provost of what town, as well as constructing MacLellan's Castle in the center of that town?
  Are Lycoris and Ammi both native to southern Asia?
  egends of the Fall was based on a novella by what poetry and fiction writer?
  Best regards
  Daniel Jones
  (555) 123-4567",

 (555) 123-4567"

"""

response_privacy = extractor.mark_privacy(letter)
print(response_privacy)
from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio


class ExpertCooperation:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.expert_handler = Columbus.experts

    async def elector_expert(self, results,question):
        try:
            tasks = []
            print("\033[1;32m------------------------------------ Expert analysis ----------------------------------------\033[0m")
            for title, score in results:
                print(f"Title: {title}, Relevance Score: {score}")
                experts = self.expert_handler.format(field=title, score=score,question=question)
                task = self.runner.execute(experts)
                tasks.append(task)

            # Await all scheduled tasks
            responses = await asyncio.gather(*tasks)
            print("\033[1;32m------------------------------------ Expert analysis_end ----------------------------------------\033[0m")
            # Extract and return the relevant messages from all responses
            related_messages = responses

            return related_messages
        except Exception as e:
            print(f"An error occurred: {e}")
            return Nonefrom src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
from src.Agents.roles.logic_math import *
import asyncio
import re


class PlanerLogic:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.math_logic = math_logic
        self.spp_plus = spp_prompt
        self.check_logic = check_logic
        self.pattern = r"//(.+?)//"
        self.c_prompt =cond_prompt
        self.think_again=think_again
    async def logic(self, question):
        try:
            all_content = self.math_logic.format(question=question)
            print("\033[1;32m------------------------------------ LOGIC THINKING ----------------------------------------\033[0m")

            # 并行执行多个异步操作

            related_message = await self.runner.execute(all_content)
            print("Answer:",related_message)
            print("\033[1;32m------------------------------------ THINK_END ----------------------------------------\033[0m")
            return related_message
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

    def analyze_text(self, text):  # 修改这里，去掉async
        matches = re.findall(self.pattern, text, re.IGNORECASE)
        combined_sentence = ' '.join(matches).lower()
        if "correct" in combined_sentence:
            return True
        elif "wrong" in combined_sentence:
            return False
        return True


    async def condense(self, question,answer,related_message):
        try:
            all_content = self.c_prompt.format(question=question,answer=answer,condence=related_message)
            print("\033[1;32m------------------------------------ CONDENSE_CONTRADICTION ----------------------------------------\033[0m")
            # 并行执行多个异步操作
            cond_info = await self.runner.execute(all_content)

            print("Briefly describe the contradiction:",cond_info)
            print("\033[1;32m------------------------------------ CONDENSE_END ----------------------------------------\033[0m")

            return cond_info
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

    async def verify(self, question, answer):
        try:
            all_content = self.check_logic.format(question=question, solutions=answer)
            print("\033[1;32m------------------------------------ VERIFY_THINKING ----------------------------------------\033[0m")

            # 并行执行多个异步操作
            related_message = await self.runner.execute(all_content)
            print("VERIFY_THINK_RESULT:",related_message)
            print("------------------------------------THINKING_END-------------------------------------------")

            contradiction = self.analyze_text(
                related_message)
            contradiction_info ='no contradiction_info'
            if not contradiction:
                print("☆☆☆☆☆☆☆☆☆☆☆The answer obtained this time was evaluated to be wrong☆☆☆☆☆☆☆☆☆☆☆")
                contradiction_info = await self.condense(question,answer,related_message)

            return contradiction,contradiction_info
        except Exception as e:
            print(f"An error occurred: {e}")
            return None


    async def logic_again(self, question,answer,memory):
        try:
            all_content = self.think_again.format(question=question,answer=answer,memory=memory)
            print("\033[1;32m------------------------------------ LOGIC_THINKING_AGAIN ----------------------------------------\033[0m")

            # 并行执行多个异步操作

            related_message = await self.runner.execute(all_content)
            print("THINK AGAIN",related_message)
            print("\033[1;32m------------------------------------ THINKING_END ----------------------------------------\033[0m")

            return related_message
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

math_logic ='''
When faced with a task, begin by identifying the participants who will contribute to solving the task. Note that the participants can only be either AI Assistant (you) or Expert. Then, initiate a multi-round collaboration process until a final conclusion is reached.  The Expert will give critical comments and detailed suggestions whenever necessary.  
 
Here are some examples:
---
Example Task 1: Use numbers and basic arithmetic operations (+ - * /) to obtain 24. You need to use all numbers, and each number can only be used once.  
Input: 6 12 1 1  
 
Participants: AI Assistant (you); Expert  
 
Start collaboration!  
 
Expert: Let's analyze the task in detail. You need to make sure that you meet the requirement, that you need to use exactly the four numbers (6 12 1 1) to construct 24. To reach 24, you can think of the common divisors of 24 such as 4, 6, 8, 3 and try to construct these first. Also you need to think of potential additions that can reach 24, such as 12 + 12.  
AI Assistant (you): Thanks for the hints! Here's one initial solution: (12 / (1 + 1)) * 6 = 24  
Expert: Let's check the answer step by step. (1+1) = 2, (12 / 2) = 6, 6 * 6 = 36 which is not 24! The answer is not correct. Can you fix this by considering other combinations? Please do not make similar mistakes.  
AI Assistant (you): Thanks for pointing out the mistake. Here is a revised solution considering 24 can also be reached by 3 * 8: (6 + 1 + 1) * (12 / 4) = 24.  
Expert: Let's first check if the calculation is correct. (6 + 1 + 1) = 8, 12 / 4 = 3, 8 * 3 = 24. The calculation is correct, but you used 6 1 1 12 4 which is not the same as the input 6 12 1 1. Can you avoid using a number that is not part of the input?  
AI Assistant (you): You are right, here is a revised solution considering 24 can be reached by 12 + 12 and without using any additional numbers: 6 * (1 - 1) + 12 = 24.  
Expert: Let's check the answer again. 1 - 1 = 0, 6 * 0 = 0, 0 + 12 = 12. I believe you are very close, here is a hint: try to change the “1 - 1” to “1 + 1”.  
AI Assistant (you): Sure, here is the corrected answer:  6 * (1+1) + 12 = 24  
Expert: Let's verify the solution. 1 + 1 = 2, 6 * 2 = 12, 12 + 12 = 12. You used 1 1 6 12 which is identical to the input 6 12 1 1. Everything looks good!  
 
Finish collaboration!  
 
Final answer: 6 * (1 + 1) + 12 = 24  

---
Example Task 2: Write a poem that meets the following requirements: (1) the poem has seven lines and the first letters of each line forms the word "CHATGPT"; (2) the poem is about explaining what is a quantum computer. (3) the poem needs to be easy to understand by a ten years old kid.

Participants: AI Assistant (you); Expert

Expert: Make sure that you write the poem with seven lines, and the first letters of the lines should be C, H, A, T, G, P, T. A quantum computer is an advanced computing device that uses the principles of quantum mechanics to process and store information. Unlike classical computers that use bits to represent information as 0s and 1s, quantum computers use quantum bits or qubits. Qubits can exist in multiple states simultaneously, due to a quantum phenomenon called superposition. You can consider using these information for the poem. I hope the poem to be fun and easy to understanding. I don't want to see a lot of jargons or complicated concepts.
AI Assistant (you): Thanks for the guidance! Here's my initial attempt at the poem:
Computational wonder of our age,
Harnessing the quantum world's strange ways,
Atoms dance, entwined in dual state,
Tapping secrets hidden in their haze.

Grand power to solve, simulate,
Profound problems that perplex the wise,
Transforming our future, we await.

Expert: Let's verify if the poem meets the requirements. The first letters are CHATGPT which is correct! And the poem rhymes well. Good job! I don't know what does perplex mean. Can you make the use of words easier to understand?
AI Assistant (you): Sure, let me revise the poem by using more common words. Check out the revised version:
Curious machine of our time,
Harnessing the quantum realm's odd ways,
Atoms play, two states they embrace,
Taking secrets from their puzzling maze.

Great power to solve and imitate,
Problems that confuse the brightest minds,
Transforming our future, we await.

Expert: Let's check again if the poem meets the requirements. The first letters are C H A T G P T. And now the poem is more accessible to children. Everything looks good to me. I like this version a lot!

Finish collaboration!

Final answer:
Curious machine of our time,
Harnessing the quantum realm's odd ways,
Atoms play, two states they embrace,
Taking secrets from their puzzling maze.

Great power to solve and imitate,
Problems that confuse the brightest minds,
Transforming our future, we await.

---

Now, identify the participants and collaboratively solve the following task step by step. Note that the participants can only be either AI Assistant (you) or Expert. Remember to provide the final solution with the following format //"Final answer: The result.//".


Task: {question}
   '''
check_logic ='''

You are an AI assistant that helps people find information.
I will provide the problem and its solution.  your task is to help me verify if this solution is indeed the correct answer to the problem. 
Below is an example of how you should structure your reasoning. Please follow this example in your validation process."
Certainly, let's simulate a scenario where two characters, **AI Assistant** and **Expert**, handle the problem. This dialog format allows us to validate each step with an extra layer of error checking.
Here is an example,Please follow this example in your validation process.
The expert does not need to make inferences, but only needs to re-evaluate the assistant's answer. If the expert thinks the assistant's evaluation is wrong, the assistant needs to answer again.
The verification process is to put the solution into the Clues to see if there is any contradiction with the question. If so, the answer is wrong.
I am not asking you to deduce the answer, but to use the solution I give as a clue and substitute it into other clues and questions to see if there is any contradiction.
### Example Structure for Validation Process

**Problem Statement**: Verify the correctness of a given solution using other provided clues and equations.

**Given Solution**: 
x = 3

**Provided Equations and Clues**:
1. \( x + 3 = 5 \)
2. \( 2x - 4 = 2 \)
3. \( x - 1 = 1 \)

### Validation Steps:

#### Verifying Equation \( x + 3 = 5 \)

**AI Assistant**: First, I'll substitute \( x = 3 \) into the first equation. Let's calculate: \( 3 + 3 \).

**Expert**: The result is 6, which is incorrect since it does not satisfy \( x + 3 = 5 \).

#### Verifying Equation \( 2x - 4 = 0 \)

**AI Assistant**: Next, I'll substitute \( x = 3 \) into the second equation. Calculating: \( 2 * 3 - 4 \).

**Expert**: The result is 2, which satisfies the equation \( 2x - 4 = 2 \). Well done, this confirms that \( x = 3 \) is correct for the second equation. Let's check the final equation.

#### Verifying Equation \( x - 1 = 1 \)

**AI Assistant**: Finally, I'll substitute \( x = 3 \) into the third equation. Calculating: \( 3 - 1 \).

**Expert**: The result is 2, which is incorrect for \( x - 1 = 1 \).

### Conclusion:

**Expert**: Based on our calculations and verifications, \( x = 3 \) does not consistently satisfy all equations. Therefore, we can confirm this as an incorrect solution. //your solution is Wrong//

**AI Assistant**: Thank you for your verification and confirmation. This collaborative approach ensures our calculations are accurate and transparent. If there are more scenarios to explore or other questions, please let me know!

now,identify the participants and collaboratively solve the following task step by step. Note that the participants can only be either AI Assistant (you) or Expert. Remember to provide the final solution with the following format "//your question is  correct// or //your question is  Wrong//".
Here is what you need to verify your answers and results:
If you find that you use the solution I provided as a clue combined with other clues to substitute into the question, If the solution is correct, the reply is //your solution is  correct//, If the solution is incorrect, the reply is //your solution is  Wrong//.
Solutions:{solutions}
Task:{question}

'''



spp_prompt = '''When faced with a task, begin by identifying the participants who will contribute to solving the task. Then, initiate a multi-round collaboration process until a final solution is reached. The participants will give critical comments and detailed suggestions whenever necessary.

Here are some examples:
---
Example Task 1: Use numbers and basic arithmetic operations (+ - * /) to obtain 24. You need to use all numbers, and each number can only be used once.
Input: 6 12 1 1

Participants: AI Assistant (you); Math Expert

Start collaboration!

Math Expert: Let's analyze the task in detail. You need to make sure that you meet the requirement, that you need to use exactly the four numbers (6 12 1 1) to construct 24. To reach 24, you can think of the common divisors of 24 such as 4, 6, 8, 3 and try to construct these first. Also you need to think of potential additions that can reach 24, such as 12 + 12.
AI Assistant (you): Thanks for the hints! Here's one initial solution: (12 / (1 + 1)) * 6 = 24
Math Expert: Let's check the answer step by step. (1+1) = 2, (12 / 2) = 6, 6 * 6 = 36 which is not 24! The answer is not correct. Can you fix this by considering other combinations? Please do not make similar mistakes.
AI Assistant (you): Thanks for pointing out the mistake. Here is a revised solution considering 24 can also be reached by 3 * 8: (6 + 1 + 1) * (12 / 4) = 24.
Math Expert: Let's first check if the calculation is correct. (6 + 1 + 1) = 8, 12 / 4 = 3, 8 * 3 = 24. The calculation is correct, but you used 6 1 1 12 4 which is not the same as the input 6 12 1 1. Can you avoid using a number that is not part of the input?
AI Assistant (you): You are right, here is a revised solution considering 24 can be reached by 12 + 12 and without using any additional numbers: 6 * (1 - 1) + 12 = 24.
Math Expert: Let's check the answer again. 1 - 1 = 0, 6 * 0 = 0, 0 + 12 = 12. I believe you are very close, here is a hint: try to change the "1 - 1" to "1 + 1".
AI Assistant (you): Sure, here is the corrected answer:  6 * (1+1) + 12 = 24
Math Expert: Let's verify the solution. 1 + 1 = 2, 6 * 2 = 12, 12 + 12 = 12. You used 1 1 6 12 which is identical to the input 6 12 1 1. Everything looks good!

Finish collaboration!

Final answer: 6 * (1 + 1) + 12 = 24

---
Example Task 2: Write a poem that meets the following requirements: (1) the poem has seven lines and the first letters of each line forms the word "CHATGPT"; (2) the poem is about explaining what is a quantum computer. (3) the poem needs to be easy to understand by a ten years old kid.

Participants: AI Assistant (you); Poet; Computer Scientist; Ten year old child

Poet: Make sure that you write the poem with seven lines, and the first letters of the lines should be C, H, A, T, G, P, T.
Computer Scientist: A quantum computer is an advanced computing device that uses the principles of quantum mechanics to process and store information. Unlike classical computers that use bits to represent information as 0s and 1s, quantum computers use quantum bits or qubits. Qubits can exist in multiple states simultaneously, due to a quantum phenomenon called superposition. You can consider using these information for the poem.
Ten year old child: I hope the poem to be fun and easy to understanding. I don't want to see a lot of jargons or complicated concepts.
AI Assistant (you): Thanks for the guidance! Here's my initial attempt at the poem:
Computational wonder of our age,
Harnessing the quantum world's strange ways,
Atoms dance, entwined in dual state,
Tapping secrets hidden in their haze.

Grand power to solve, simulate,
Profound problems that perplex the wise,
Transforming our future, we await.

Poet: Let's verify if the poem meets the requirements. The first letters are CHATGPT which is correct! And the poem rhymes well. Good job!
Computer Scientist: Everything looks good to me!
Ten year old child: I don't know what does perplex mean. Can you make the use of words easier to understand?
AI Assistant (you): Sure, let me revise the poem by using more common words. Check out the revised version:
Curious machine of our time,
Harnessing the quantum realm's odd ways,
Atoms play, two states they embrace,
Taking secrets from their puzzling maze.

Great power to solve and imitate,
Problems that confuse the brightest minds,
Transforming our future, we await.

Poet: Let's check again if the poem meets the requirements. The first letters are C H A T G P T. And now the poem is more accessible to children. Everything looks good to me.
Computer Scientist: Looking good!
Ten year old child: I like this version a lot!

Finish collaboration!

Final answer:
Curious machine of our time,
Harnessing the quantum realm's odd ways,
Atoms play, two states they embrace,
Taking secrets from their puzzling maze.

Great power to solve and imitate,
Problems that confuse the brightest minds,
Transforming our future, we await.

---
Now, identify the participants and collaboratively solve the following task step by step. Remember to present your final solution with the prefix "Final answer:".

Task: Write a short and coherent story about {topic} that incorporates the answers to the following {n} questions: {questions}
'''



cond_prompt = '''
I have a question for which I found an answer using a method, but then a model revealed a contradiction between the question and the answer.
question:{question}
answer:{answer}
contradiction:{condence}
 This is the analysis result.
  Please help me summarize the contradiction in fifty words or less!
  Please help me summarize the contradiction in fifty words or less!
  You only need to answer the conflicting content briefly.
'''


think_again ='''

    Below is an example of how you should structure your reasoning. Please follow this example in your validation process."
    Certainly, let's simulate a scenario where two characters, **AI Assistant** and **Expert**, handle the problem. This dialog format allows us to validate each step with an extra layer of error checking.
    Here is an example,Please follow this example in your validation process.
    The expert does not need to make inferences, but only needs to re-evaluate the assistant's answer. If the expert thinks the assistant's evaluation is wrong, the assistant needs to answer again.
    **Given Solution**:
        x = 2 
    **Provided Equations and Clue**:

         x + 3 = 5 
         2x - 4 = 0 
         x - 1 = 1 

    ### Validation Steps:

    #### Verifying Equation \( x + 3 = 5 \)

    **AI Assistant**: First, I'll substitute \( x = 2 \) into the first equation. Let's calculate: \( 2 + 3 = 5 \).

    **Expert**: The result is 5, which matches the right side of the equation. This confirms that the solution is correct for equation one. Please proceed to the next equation.

    #### Verifying Equation \( 2x - 4 = 0 \)

    **AI Assistant**: Next, I'll substitute \( x = 2 \) into the second equation. Calculating: \( 2 \times 2 - 4 = 0 \).

    **Expert**: The result is 0, which satisfies the condition of the equation. Well done, this confirms that \( x = 2 \) is also correct for the second equation. Now, let's verify the final equation.

    #### Verifying Equation \( x - 1 = 1 \)

    **AI Assistant**: Finally, I'll substitute \( x = 2 \) into the third equation. Calculating: \( 2 - 1 = 1 \).

    **Expert**: The result is also 1, which exactly meets the expectations of the equation. This means \( x = 2 \) is valid in all provided equations.

    ### Conclusion:

    **Expert**: Based on our calculations and verifications, \( x = 2 \) has been correctly validated across all equations. Therefore, we can confirm this as the correct solution.

    **AI Assistant**: Thank you for your verification and confirmation. This collaborative approach ensures our calculations are free from errors.

    This dialogue-style validation method not only makes the process clear but also adds an extra layer of checking through the division of roles, helping to ensure accuracy. If you need further assistance or have more questions, please let me know!

    now,identify the participants and collaboratively solve the following task step by step. Note that the participants can only be either AI Assistant (you) or Expert. Remember to provide the final solution with the following format "//Final answer: The house number here.//".
---
Now I will give you this question: {question}, and there is a answer:{answer}, but after analysis, **this answer is incorrect because **{memory}**.You must find a new solution.
Now, identify the participants and collaboratively solve the following task step by step. Remember to present your final solution with the prefix "//"Final answer: The result.//".
'''from src.LLM_Model.Llama_8B import TextGenerator


class DocumentEntityExtractor:
    def __init__(self, model_path="/home/zhaorh/code/ppagent/model/Llama-7B"):
        self.generator = TextGenerator(model_path)

        self.messages_name = [{"role": "user",
                               "content": "Extract the names of people mentioned in the following document. If there are no names, respond with 'No names found.'"}]
        self.messages_location = [{"role": "user",
                                   "content": "Extract the place names mentioned in the following document. If there are no place names, respond with 'No place names found.'"}]
        self.messages_organization = [{"role": "user",
                                       "content": "Extract the organization names mentioned in the following document. If there are no organization names, respond with 'No organization names found.'"}]
        self.message_mark_privacy = [{"role": "user",
                                      "content": "Extract the private data of the letter, such as the name of the writer, the recipient, where it comes from, and the organization"}]

    def extract_entities(self, doc):
        response_name = self.generator.generate_text(self.messages_name, doc)
        print('name',response_name)
        response_location = self.generator.generate_text(self.messages_location, doc)
        print('location', response_location)
        response_organization = self.generator.generate_text(self.messages_organization, doc)
        print('organization', response_organization)
        return response_name, response_location, response_organization

    def mark_privacy(self, letter):
        response_privacy = self.generator.generate_text(self.message_mark_privacy, letter)
        return response_privacy


# 使用示例
extractor = DocumentEntityExtractor()

document = """Were Thinking Fellers Union Local 282 and the Smiths, who had a lead singer of Morrissey, both active in 1986?"""
response_name, response_location, response_organization = extractor.extract_entities(document)

# 示例信件
letter = """
Dear Emma Williams,
I am from Denver, and I am a part of the Rocky Mountain Historical Society.
 I would like to ask you a few questions.
  John Coffin Talbot served as a legislator in the Maine Legislature, Where does this state's legislature convene?
  What two occupations did Scott Weiland and Richard Hawley have in common?
  Thomas Maclellan of Bombie was Provost of what town, as well as constructing MacLellan's Castle in the center of that town?
  Are Lycoris and Ammi both native to southern Asia?
  egends of the Fall was based on a novella by what poetry and fiction writer?
  Best regards
  Daniel Jones
  (555) 123-4567",

 (555) 123-4567"
   
"""

response_privacy = extractor.mark_privacy(letter)
print(response_privacy)
from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio

class ProblemClassification:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.cls = '''
        Consider the following text sample and its attributes. If the text is structured to involve reasoning through clues or indirect hints, respond with "no". If the question requires background knowledge to answer, respond with "yes.".
        Here are two examples. 
        For example, if you are asked who will be the president of the United States in 2010, the answer is Obama. 
        Obviously, the question does not contain the answer, and you need to obtain background knowledge. 
        In this case, you should answer yes. When the question is A is greater than B, B is greater than C, and you ask whether A is greater than C? 
        This involves reasoning, and does not require background knowledge, so you should answer no.
        question: {question}"
        Based on the description and the sample, determine the appropriate response and provide either "no" or "yes".'''
    async def repair(self, question):
        try:
            all_content = self.cls.format(question=question)
            print("\033[1;32m------------------------------------ GET knowledge ----------------------------------------\033[0m")

            yes_count = 0
            no_count = 0

            # 执行三次finally_answer
            for i in range(3):
                finally_answer = await self.runner.execute(all_content)
                print(f"☆☆☆☆☆ Classification result {i+1}: {finally_answer}")

                # 统计yes和no的出现次数
                if 'yes' in finally_answer.lower():
                    yes_count += 1
                elif 'no' in finally_answer.lower():
                    no_count += 1

            print("\033[1;32m------------------------------------ GET_END ----------------------------------------\033[0m")

            # 根据统计结果返回yes或no
            if yes_count >= 2:
                print("External knowledge is required (yes).")
                return 'yes'
            elif no_count >= 2:
                print("External knowledge is not required (no).")
                return 'no'
            else:
                print("Could not determine if external knowledge is required.")
                return "Unknown response"

        except Exception as e:
            print(f"An error occurred: {e}")
            return None
from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio



class RepairPerson:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.repairman = Columbus.repairman

    async def repair(self, question,results,all_answer):
        try:
            content = ""
            number = 1
            for result in results:
                content = content + str(number) + ':' + result
                number += 1
            all_content = self.repairman.format(Question=question,answer=all_answer,content=content)
            print("------------------------------------repair_person----------------------------------------")

            # 并行执行多个异步操作
            finally_answer = await self.runner.execute(all_content)
            print("------------------------------------repair_end----------------------------------------")

            return finally_answer
        except Exception as e:
            print(f"An error occurred: {e}")
            return None


import re

from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio

class ExpertSelector:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.expert_handler = Columbus.analyzy

    async def elector_expert(self, collect_information):
        try:
            experts = self.expert_handler.format(collect_information=collect_information)

            print("------------------------------------collect_information----------------------------------------")

            # Execute the query asynchronously
            related_message = await self.runner.execute(experts)
            print("------------------------------------collect_end----------------------------------------")

            # Extract and return the relevant message from the response

            return related_message
        except Exception as e:
            print(f"An error occurred: {e}")
            return None


class IndustryExtractor:
    def __init__(self, text):
        self.text = text

    def extract(self):
        # 使用正则表达式来查找标题和相关度分数
        pattern = r'\*\*(.*?)\*\*\s*Relevance\s*Score:\s*(\w+(?:\s\w+)*)'
        # 查找所有符合条件的匹配
        results = re.findall(pattern, self.text, re.IGNORECASE | re.DOTALL)

        # 移除标题中不需要的单词'Industry'
        cleaned_results = [(title.replace('Industry', '').strip(), score) for title, score in results]
        return cleaned_results
from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio
import re


def extract_and_check(text):
    try:
        # 查找第一个 // 的位置
        start = text.find("//")
        if start != -1:
            # 查找第二个 // 的位置
            end = text.find("//", start + 2)
            if end != -1:
                # 提取第一个 // 之间的内容
                content = text[start + 2:end].strip()

                # 检查内容是否包含 "correct"
                if "correct" in content:
                    return True
                elif "wrong" in content:
                    # 检查后续内容是否包含 "answer:"
                    answer_start = text.find("answer：", end)
                    if answer_start != -1:
                        # 提取 "answer:" 后面的内容
                        answer_content = text[answer_start + len("answer:"):].strip()
                        return False, answer_content
                    else:
                        return False, "No answer found"
        return "No content found"

    except Exception as e:
        print(f"An error occurred: {e}")
        return None


class SubCheckMan:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.repairman = Columbus.sub_repairman

    async def check(self, question,answer):
        try:
            all_content = self.repairman.format(question=question,answer=answer)
            print("\033[1;32m------------------------------------ Sub_question-Check ----------------------------------------\033[0m")

            # 并行执行多个异步操作
            output =   await self.runner.execute(all_content)

            print(output)
            result2 = extract_and_check(output)
            print("\033[1;32m------------------------------------ Sub_question-Check-End ----------------------------------------\033[0m")
            return result2
        except Exception as e:
            print(f"An error occurred: {e}")
            return None

import asyncio
from src.Agents.roles.logic_expert import PlanerLogic
from src.Agents.roles.problem_classification_major import ProblemClassification
from src.Agents.find_answer import FinalAnswerExtractor
from src.Agents.planer import Planner
import re
class QuestionProcessor:
    def __init__(self):
        self.problem_classifier = ProblemClassification()
        self.phrase ="Final answer:"

    def extract_text(self, text):
        # 使用正则表达式从特定短语到 // 之间匹配内容
        pattern = re.escape(self.phrase) + r"(.+?)//"
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()  # 返回匹配的内容，并去除两端的空白字符
        return "no answer"  # 如果没有找到匹配，返回空字符串



    async def process_question(self, question,id_=None):
        result = await self.problem_classifier.repair(question)
        if result == 'yes':
            # 如果需要外部知识，则可以在这里执行相关操作
             planner = Planner(question)
             fusion_answers,answers,final_answer = await planner.run()
             return fusion_answers,answers,final_answer
        elif result == 'no':

            #logic = PlanerLogic()
            #省略步骤

            #file_path = '/mnt/c/workspace/code/xagent_zhao/data/logic_grid_puzzle/logic_grid_puzzle_200.jsonl__method-spp_engine-devgpt4-32k_temp-0.0_topp-1.0_start0-end200__without_sys_mes.jsonl'
            #extractor = FinalAnswerExtractor(file_path)
            #answer =extractor.get_answer_by_index(id_) # Get the first answer
            #print("☆☆☆☆☆☆First Answer:",answer)
            logic = PlanerLogic()
            answer = await logic.logic(question)
            answer_is,contradiction_info = await logic.verify(question,answer)
            if answer_is:
                return answer,answer,answer
            else:
                for i in range(3):
                    memory = str(contradiction_info)
                    answer = await logic.logic_again(question,answer,memory)
                    answer = self.extract_text(answer)
                    print("☆☆☆☆☆new answer:",answer)
                    answer_is,contradiction_info=await logic.verify(question,answer)
                    if answer_is:
                        break
            return answer,answer,answer



        else:
            print("Unable to determine if external knowledge is required")
            answer = None  # 确保 answer 被定义，无论结果如何
            return answer

import asyncio
from src.text_analysis.TF_IDF import TextDomainAnalyzer

class DomainScoreCalculator:
    def __init__(self):
        """
        初始化类，创建 TextDomainAnalyzer 实例，并定义领域列表。
        """
        self.analyzer = TextDomainAnalyzer()
        self.domains = ["Entertainment", "financial", "History", "legal", "Literature",
                        "medical", "Politics", "Sports", "technology", "science"]

    async def calculate_domain_scores(self, sample_text):
        """
        计算给定文本在各个领域的归属分数。

        参数:
        - sample_text: 要计算的文本。

        返回:
        - 一个字典，键是领域名称，值是归属分数。
        """
        domain_scores = {}

        # 逐个领域计算归属分数
        for domain in self.domains:
            result = await self.analyzer.domain_membership_score_normalized(sample_text, domain)
            domain_scores[result[0]] = result[1]

        return domain_scores

    async def get_domain_scores(self, sample_text):
        """
        计算并返回每个领域的归属分数。

        参数:
        - sample_text: 要计算的文本。

        返回:
        - 一个字典，包含每个领域的归属分数。
        """
        domain_scores = await self.calculate_domain_scores(sample_text)
        return domain_scores
from src.Agents.roles import Columbus
from src.Agents.API_info import ChatActionRunner
import asyncio
import json
import asyncio
import aiofiles
from src.Agents.start_Q import QuestionProcessor
from src.Agents.memory import config_w2v
from src.Agents.privacy_part import privacy_part
config_w2v.initialize_calculator()
from src.LLM_pp_agent import LLMpp
import re
from src.Evaluation import *


async def main():
    question = "aswdadas"
    processor = QuestionProcessor()
    line_number = None
    plan_answer, answers, final_answer = await processor.process_question(question, line_number)




# 运行异步任务
asyncio.run(main())from transformers import BertTokenizer, BertModel
import torch
from nltk.translate.bleu_score import sentence_bleu




import re
# 初始化 BERT 模型和分词器
tokenizer = BertTokenizer.from_pretrained('/home/zhaorh/code/ppagent/model/bert-base-uncased')
model = BertModel.from_pretrained('/home/zhaorh/code/ppagent/model/bert-base-uncased')

def get_embedding(text):
    inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)

def cosine_similarity(embedding1, embedding2):
    return torch.nn.functional.cosine_similarity(embedding1, embedding2).item()

def calculate_f1(hidden_count, total_hidden, non_hidden_count, total_non_hidden):
    precision = hidden_count / total_hidden if total_hidden > 0 else 0
    recall = hidden_count / non_hidden_count if non_hidden_count > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    return f1

def evaluate_bleu(original_text, masked_text):
    reference = original_text.split()
    candidate = masked_text.split()
    return sentence_bleu([reference], candidate)

def evaluate_semantic_similarity(original_text, masked_text):
    embedding_original = get_embedding(original_text)
    embedding_masked = get_embedding(masked_text)
    return cosine_similarity(embedding_original, embedding_masked)

def evaluate_f1(hidden_privacy_count, total_privacy_count, non_privacy_count, false_hidden_count):
    return calculate_f1(hidden_privacy_count, total_privacy_count, non_privacy_count, false_hidden_count)


def update_privacy_scores(scores, scores_2, text, privacy_data):
    """
    根据文本内容和隐私数据更新分数，记录未匹配的隐私项数量。

    参数：
    - scores: 更新后的分数字典，记录未匹配的隐私项数量
    - scores_2: 统计每类隐私项的总数
    - text: 要检测的文本内容（可能已被加密）
    - privacy_data: 包含隐私数据的字典结构

    返回：
    - 更新后的 scores 和 scores_2 字典
    """
    # 检查参数类型
    if not isinstance(text, str):
        raise ValueError("输入的文本（text）必须是字符串类型")
    if not isinstance(privacy_data, dict):
        raise ValueError("隐私数据（privacy_data）必须是字典类型")

    matched_items = set()  # 记录已匹配的隐私项

    # 遍历每一类隐私数据
    for category, items in privacy_data.items():
        # 确保 scores_2 累加该类别的总数
        if category not in scores_2:
            scores_2[category] = 0
        scores_2[category] += len(items)

        # 初始化该类别的分数，如果不存在则置为 0
        if category not in scores:
            scores[category] = 0

        # 遍历该类别的每一个数据项
        for item in items:
            # 检查 item 是否是字符串类型
            if not isinstance(item, str) or not item.strip():
                print(f"跳过无效数据项：{item} 在类别：{category}")
                continue

            try:
                # 使用正则表达式匹配，忽略大小写和空格差异
                pattern = re.compile(re.escape(item.strip()), re.IGNORECASE)
                if not pattern.search(text):
                    scores[category] += 1  # 未匹配到则计数加 1
                    print(f"未匹配到：{item} 在类别：{category}")
                else:
                    print(f"匹配到：{item} 在类别：{category}")
            except re.error as e:
                print(f"正则表达式错误：{e}，跳过数据项：{item}")

    return scores, scores_2




def count_non_privacy_words(text, privacy_data):
    """
    统计文本中除了隐私数据外的其它单词数量。

    参数：
    - text: 输入的文本字符串。
    - privacy_data: 包含隐私数据的字典。

    返回：
    - 非隐私单词的数量。
    """

    # 使用正则表达式提取文本中的所有单词
    words = re.findall(r'\b\w+\b', text)

    # 将所有隐私数据项收集到一个集合中
    privacy_words = set()
    for category, items in privacy_data.items():
        privacy_words.update(items)

    # 过滤掉隐私数据中的单词
    non_privacy_words = [word for word in words if word not in privacy_words]

    # 返回非隐私单词的数量
    return len(non_privacy_words)

def count_nested_answers_in_text(text, nested_answers):
    """
    计算文本中包含给定答案字典中答案的个数。
    每个子列表视为一个单独的得分单位，如果文本包含子列表中的任何一个答案，则得分加一。
    使用正则表达式确保匹配完整单词边界，并处理数字后缀（如 "12th" 匹配 "12"）。

    参数:
    - text (str): 需要检查的文本。
    - nested_answers (dict): 嵌套答案字典，每个键对应一个问题，值为包含多个可能答案的列表。

    返回:
    - int: 总得分。
    """
    lower_text = text.lower()  # 将文本转换为小写，方便匹配
    total_score = 0  # 初始化总得分
    answer_index = 1  # 用于标识当前答案编号

    # 遍历每个问题的答案列表
    for key, answers in nested_answers.items():
        found = False  # 标识是否在当前问题中找到匹配答案

        # 遍历该问题的所有可能答案
        for answer_set in answers:
            words = answer_set.lower().split()  # 拆分答案为单词列表

            # 检查答案中的所有单词是否都能在文本中找到
            if all(re.search(r'\b' + re.escape(word) + r'[a-z]*s?\b', lower_text) for word in words):
                # 如果答案包含数字，处理数字后缀
                if any(word.isdigit() for word in words):
                    if all(re.search(r'\b' + re.escape(word) + r'(?:st|nd|rd|th)?\b', lower_text) for word in words):
                        print(f'The {answer_index} correct answer is identified. The correct answer is {answer_set}')
                        found = True
                        break
                else:
                    print(f'The {answer_index} correct answer is identified. The correct answer is {answer_set}')
                    found = True
                    break

        if found:
            total_score += 1
        else:
            print(
                f'The correct answer was not identified for the {answer_index} one, and the solution does not have the following answer: {answers}')

        answer_index += 1  # 增加答案编号

    return total_score



def extract_words(text):
    """从文本中提取所有单词，将连字符拆分为多个单词。"""
    # 将连字符替换为空格，然后提取单词
    text = text.replace('-', ' ')
    words = re.findall(r'\b\w+\b', text.lower())
    return set(words)



def filter_privacy_words(words, privacy_data):
    """从单词集合中排除隐私数据中的词汇。"""
    privacy_words = set()
    for category, items in privacy_data.items():
        privacy_words.update(item.lower() for item in items)  # 将所有隐私词汇转换为小写
    return words - privacy_words  # 返回去除隐私词汇后的集合

def count_missing_words(text1, text2, privacy_data):
    """
    统计 text1 中在 text2 中未出现的单词个数（排除隐私数据）。

    参数：
    - text1: 原始文本。
    - text2: 用于比较的文本。
    - privacy_data: 隐私数据字典。

    返回：
    - 未出现在 text2 中的单词数量。
    - 未出现在 text2 中的单词列表。
    """
    # 提取两个文本的单词集合
    words1 = extract_words(text1)
    words2 = extract_words(text2)

    # 从单词集合中排除隐私数据词汇
    words1_filtered = filter_privacy_words(words1, privacy_data)
    words2_filtered = filter_privacy_words(words2, privacy_data)

    # 找出 text1 中不在 text2 中的单词
    missing_words = words1_filtered - words2_filtered

    # 返回未出现单词的数量和列表
    return len(missing_words), missing_words



def process_privacy_scores_and_metrics(
    scores_ner, scores_2_ner,
    scores_llm, scores_2_llm,
    scores_mask, scores_2_mask,
    NER_question, LLM_masked_question, masked_question,
    data, task_all,
    ner_bleu_scores, ner_semantic_similarities,
    llm_bleu_scores, llm_semantic_similarities,
    mask_bleu_scores, mask_semantic_similarities,
    missing_count_ner_sum, non_privacy_count_ner_sum,
    missing_count_llm_sum, non_privacy_count_llm_sum,
    missing_count_mask_sum, non_privacy_count_mask_sum
):
    # 更新隐私得分
    scores_ner, scores_2_ner = update_privacy_scores(scores_ner, scores_2_ner, NER_question, data.get('privacy_data'))
    scores_llm, scores_2_llm = update_privacy_scores(scores_llm, scores_2_llm, LLM_masked_question, data.get('privacy_data'))
    scores_mask, scores_2_mask = update_privacy_scores(scores_mask, scores_2_mask, masked_question, data.get('privacy_data'))

    # 计算非隐私词的数量
    non_privacy_count_ner = count_non_privacy_words(NER_question, data.get('privacy_data'))
    non_privacy_count_llm = count_non_privacy_words(LLM_masked_question, data.get('privacy_data'))
    non_privacy_count_mask = count_non_privacy_words(masked_question, data.get('privacy_data'))

    # 计算丢失的词数量和列表
    missing_count_ner, missing_words_ner = count_missing_words(task_all, NER_question, data.get('privacy_data'))
    missing_count_llm, missing_words_llm = count_missing_words(task_all, LLM_masked_question, data.get('privacy_data'))
    missing_count_mask, missing_words_mask = count_missing_words(task_all, masked_question, data.get('privacy_data'))

    # 评估 BLEU 和语义相似度
    bleu_score_ner = evaluate_bleu(task_all, NER_question)
    semantic_similarity_ner = evaluate_semantic_similarity(task_all, NER_question)
    ner_bleu_scores.append(bleu_score_ner)
    ner_semantic_similarities.append(semantic_similarity_ner)

    bleu_score_llm = evaluate_bleu(task_all, LLM_masked_question)
    semantic_similarity_llm = evaluate_semantic_similarity(task_all, LLM_masked_question)
    llm_bleu_scores.append(bleu_score_llm)
    llm_semantic_similarities.append(semantic_similarity_llm)

    bleu_score_mask = evaluate_bleu(task_all, masked_question)
    semantic_similarity_mask = evaluate_semantic_similarity(task_all, masked_question)
    mask_bleu_scores.append(bleu_score_mask)
    mask_semantic_similarities.append(semantic_similarity_mask)

    # 打印结果
    print("ner", bleu_score_ner, semantic_similarity_ner,
          "llm", bleu_score_llm, semantic_similarity_llm,
          "mask", bleu_score_mask, semantic_similarity_mask)

    # 累加统计结果
    missing_count_ner_sum += missing_count_ner
    non_privacy_count_ner_sum += non_privacy_count_ner
    missing_count_llm_sum += missing_count_llm
    non_privacy_count_llm_sum += non_privacy_count_llm
    missing_count_mask_sum += missing_count_mask
    non_privacy_count_mask_sum += non_privacy_count_mask

    # 返回所有更新后的变量
    return (
        scores_ner, scores_2_ner,
        scores_llm, scores_2_llm,
        scores_mask, scores_2_mask,
        ner_bleu_scores, ner_semantic_similarities,
        llm_bleu_scores, llm_semantic_similarities,
        mask_bleu_scores, mask_semantic_similarities,
        missing_count_ner_sum, non_privacy_count_ner_sum,
        missing_count_llm_sum, non_privacy_count_llm_sum,
        missing_count_mask_sum, non_privacy_count_mask_sum
    )



def calculate_and_print_metrics(
    scores_ner, scores_2_ner, missing_count_ner_sum, non_privacy_count_ner_sum,
    scores_llm, scores_2_llm, missing_count_llm_sum, non_privacy_count_llm_sum,
    scores_mask, scores_2_mask, missing_count_mask_sum, non_privacy_count_mask_sum,
    ner_bleu_scores, ner_semantic_similarities,
    llm_bleu_scores, llm_semantic_similarities,
    mask_bleu_scores, mask_semantic_similarities
):
    try:
        def weighted_metrics(hidden_count, total_privacy, false_hidden, non_privacy_count):
            print('计算F1的',hidden_count, total_privacy, false_hidden, non_privacy_count)
            # 计算隐私类的Precision、Recall和F1
            if hidden_count + false_hidden > 0:
                precision_privacy = hidden_count / (hidden_count + false_hidden)
            else:
                precision_privacy = 0.0

            if total_privacy > 0:
                recall_privacy = hidden_count / total_privacy
            else:
                recall_privacy = 0.0

            if precision_privacy + recall_privacy > 0:
                f1_privacy = 2 * precision_privacy * recall_privacy / (precision_privacy + recall_privacy)
            else:
                f1_privacy = 0.0

            # 计算非隐私类的Precision、Recall和F1
            if non_privacy_count > 0:
                precision_non_privacy = (non_privacy_count - false_hidden) / non_privacy_count
                recall_non_privacy = (non_privacy_count - false_hidden) / non_privacy_count
            else:
                precision_non_privacy = 0.0
                recall_non_privacy = 0.0

            if precision_non_privacy + recall_non_privacy > 0:
                f1_non_privacy = 2 * precision_non_privacy * recall_non_privacy / (
                            precision_non_privacy + recall_non_privacy)
            else:
                f1_non_privacy = 0.0

            # 计算权重
            total_count = total_privacy + non_privacy_count
            weight_privacy = total_privacy / total_count if total_count > 0 else 0.0
            weight_non_privacy = non_privacy_count / total_count if total_count > 0 else 0.0

            # 加权Precision、Recall和F1
            weighted_precision = (precision_privacy * weight_privacy) + (precision_non_privacy * weight_non_privacy)
            weighted_recall = (recall_privacy * weight_privacy) + (recall_non_privacy * weight_non_privacy)
            if weighted_precision + weighted_recall > 0:
                weighted_f1 = 2 * weighted_precision * weighted_recall / (weighted_precision + weighted_recall)
            else:
                weighted_f1 = 0.0

            return weighted_precision, weighted_recall, weighted_f1

        # 计算 NER 指标
        hidden_privacy_count_ner = sum(scores_ner.values())
        total_privacy_count_ner = sum(scores_2_ner.values())
        false_hidden_count_ner = missing_count_ner_sum
        non_privacy_count_ner = non_privacy_count_ner_sum

        # 计算 LLM 指标
        hidden_privacy_count_llm = sum(scores_llm.values())
        total_privacy_count_llm = sum(scores_2_llm.values())
        false_hidden_count_llm = missing_count_llm_sum
        non_privacy_count_llm = non_privacy_count_llm_sum

        # 计算 Mask 指标
        hidden_privacy_count_mask = sum(scores_mask.values())
        total_privacy_count_mask = sum(scores_2_mask.values())
        false_hidden_count_mask = missing_count_mask_sum
        non_privacy_count_mask = non_privacy_count_mask_sum

        # 打印并计算 NER 评估指标
        print("NER Metrics:")
        precision_ner, recall_ner, f1_ner = weighted_metrics(
            hidden_privacy_count_ner, total_privacy_count_ner,
            false_hidden_count_ner, non_privacy_count_ner
        )
        print(f"  Precision: {precision_ner:.3f}")
        print(f"  Recall: {recall_ner:.3f}")
        print(f"  F1 Score: {f1_ner:.3f}")
        print("-" * 50)

        # 打印并计算 LLM 评估指标
        print("LLM Metrics:")
        precision_llm, recall_llm, f1_llm = weighted_metrics(
            hidden_privacy_count_llm, total_privacy_count_llm,
            false_hidden_count_llm, non_privacy_count_llm
        )
        print(f"  Precision: {precision_llm:.3f}")
        print(f"  Recall: {recall_llm:.3f}")
        print(f"  F1 Score: {f1_llm:.3f}")
        print("-" * 50)

        # 打印并计算 Mask 评估指标
        print("Mask Metrics:")
        precision_mask, recall_mask, f1_mask = weighted_metrics(
            hidden_privacy_count_mask, total_privacy_count_mask,
            false_hidden_count_mask, non_privacy_count_mask
        )
        print(f"  Precision: {precision_mask:.3f}")
        print(f"  Recall: {recall_mask:.3f}")
        print(f"  F1 Score: {f1_mask:.3f}")
        print("-" * 50)

        # 计算并打印 BLEU 和语义相似度
        bleu_ner = sum(ner_bleu_scores) / len(ner_bleu_scores)
        similarity_ner = sum(ner_semantic_similarities) / len(ner_semantic_similarities)

        bleu_llm = sum(llm_bleu_scores) / len(llm_bleu_scores)
        similarity_llm = sum(llm_semantic_similarities) / len(llm_semantic_similarities)

        bleu_mask = sum(mask_bleu_scores) / len(mask_bleu_scores)
        similarity_mask = sum(mask_semantic_similarities) / len(mask_semantic_similarities)

        print("\nNER Evaluation:")
        print(f"  BLEU Score: {bleu_ner:.3f}")
        print(f"  Semantic Similarity: {similarity_ner:.3f}")

        print("\nLLM Evaluation:")
        print(f"  BLEU Score: {bleu_llm:.3f}")
        print(f"  Semantic Similarity: {similarity_llm:.3f}")

        print("\nMASK Evaluation:")
        print(f"  BLEU Score: {bleu_mask:.3f}")
        print(f"  Semantic Similarity: {similarity_mask:.3f}")

    except Exception as e:
        print(f"An error occurred: {e}")
import json
import asyncio
from src.Agents.start_Q import QuestionProcessor
from src.Agents.memory import config_w2v
config_w2v.initialize_calculator()
import re


def count_nested_answers_in_text(text, nested_answers):
    """
    计算文本中包含给定答案列表中的答案个数。
    每个子列表视为一个单独的得分单位，如果文本包含子列表中的任何一个答案，则得分加一。
    使用正则表达式确保匹配完整单词边界。
    特别处理类似“12th”匹配“12”的情况，以及处理部分匹配（如“answer”匹配“ans”）。

    参数:
    - text (str): 需要检查的文本。
    - nested_answers (list of list of str): 嵌套答案列表。

    返回:
    - int: 总得分。
    """
    lower_text = text.lower()
    total_score = 0
    a = 1

    for answers in nested_answers:
        found = False

        for answer_set in answers:
            words = answer_set.lower().split()
            # Check if all words in the answer are present in the text
            if all(
                re.search(r'\b' + re.escape(word) + r'[a-z]*s?\b', lower_text) for word in words
            ):
                # Special handling for numeric answers with suffixes
                if any(word.isdigit() for word in words):
                    if all(
                        re.search(r'\b' + re.escape(word) + r'(?:st|nd|rd|th)?\b', lower_text) for word in words
                    ):
                        print(f'The {a} correct answer is identified. The correct answer is {answer_set}')
                        found = True
                        break
                else:
                    print(f'The {a} correct answer is identified. The correct answer is {answer_set}')
                    found = True
                    break

        if found:
            total_score += 1
        else:
            print(f'The correct answer was not identified for the {a} one, and the solution does not have the following answer: {answers}')

        a += 1

    return total_score


async def read_jsonl_file(file_path):
    sum1 = 0
    sum2 = 0
    start_line = 1
    sum3 = 0
    """
    逐行读取 JSONL 文件，对每个样本执行答案检查，并在用户按 Enter 确认后继续。
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            for line_number, line in enumerate(file, start=1):
                if line_number < start_line:
                    continue  # Skip lines until the specified start line

                data = json.loads(line)
                processor = QuestionProcessor()
                if 'questions' in data and 'answers' in data:
                    print("NO：", line_number)
                    print("Topic:", data['topic'])
                    n = len(data['question_ids'])
                    # 假设standard.demo()返回问题的标准答案列表
                    task = 'Write a short and coherent story about {topic} that incorporates the answers to the following {n} questions: {questions}'
                    questions_str = " ".join(data['questions'])
                    task_all =task.format(topic =data['topic'],n=n,questions =questions_str)
                    print(task_all)
                    #answer_standards = await standard.demo(task_all)
                    #print('标准答案：',answer_standards)
                    #answer_standards_matches = count_nested_answers_in_text(answer_standards, data['answers'])
                    #print(f"Number of standards answers found in story: {answer_standards_matches}")
                    #sum2 = answer_standards_matches + sum2
                    #print('标准版答对的个数', sum2)
                    plan_answer,answers,final_answer = await processor.process_question(task_all, line_number)
                    print('fusion finally answer:',plan_answer)
                    print('SUB ANSWER',answers)
                    print('repair fusion finally answer:',final_answer)
                    sub_a=" ".join(answers)
                    print('+++++sub_question++++++')
                    answer_plan_matches_sub = count_nested_answers_in_text(sub_a, data['answers'])
                    print(f"SUB Number of plan answers found in story: {answer_plan_matches_sub}")
                    print('+++++plan_question++++++')
                    answer_plan_matches = count_nested_answers_in_text(plan_answer, data['answers'])
                    print(f"Number of fusion answers found in story: {answer_plan_matches}")

                    if "perfect" in  final_answer.lower():  # 不区分大小写地检查是否包含 'perfect'
                        # 执行操作 A
                        print('+++++plan_repair_question++++++')
                        answer_repair_matches=answer_plan_matches
                    else:
                        print('+++++plan_repair_question++++++')
                        answer_repair_matches = count_nested_answers_in_text(final_answer, data['answers'])
                        print(f"Number of fusion answers found in story: {answer_repair_matches}")
                    sum1 = answer_plan_matches + sum1
                    sum2 =answer_repair_matches +sum2
                    sum3 = answer_plan_matches_sub +sum3
                    print('The number of correct answers to the verification sub_questions:',sum3)
                    print('The number of correct answers after the repair:',sum2)
                    print('The number of correct answers',sum1)
    except FileNotFoundError:
        print(f"Error: File not found - {file_path}")
    except Exception as e:
        print(f"An error occurred: {e}")

async def main():
    file_path = '../data/SSP/trivia_creative_writing_100_n_5.jsonl'
    await read_jsonl_file(file_path)


# 运行主函数
asyncio.run(main())
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class TextGenerator:
    def __init__(self, model_path):
        # 初始化时加载模型和tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map="auto")

    def generate_text(self, messages, document):
        # 格式化输入文本
        formatted_input = self._get_formatted_input(messages, document)
        # Tokenize输入文本
        tokenized_prompt = self.tokenizer(self.tokenizer.bos_token + formatted_input, return_tensors="pt").to(self.model.device)
        # 设置结束符
        terminators = [self.tokenizer.eos_token_id]
        # 使用模型生成文本
        outputs = self.model.generate(
            input_ids=tokenized_prompt.input_ids,
            attention_mask=tokenized_prompt.attention_mask,
            max_new_tokens=128,
            eos_token_id=terminators
        )
        # 解码生成的文本
        response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]
        return self.tokenizer.decode(response, skip_special_tokens=True)

    def _get_formatted_input(self, messages, context):
        system = "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context."
        conversation = '\n\n'.join(
            ["User: " + item["content"] for item in messages if item['role'] == "user"]) + "\n\nAssistant:"
        formatted_input = system + "\n\n" + context + "\n\n" + conversation
        return formatted_input

# 使用
model_path = "/home/zhaorh/code/ppagent/model/Llama-7B"
generator = TextGenerator(model_path)
document = """Were Thinking Fellers Union Local 282 and the Smiths, who had a lead singer of Morrissey, both active in 1986?"""
messages = [{"role": "user", "content": "Extract the names of people from the document."}]

response = generator.generate_text(messages, document)
print(response)
from openai import OpenAI
import datetime
class ChatClient:
    def __init__(self, api_key, base_url='https://api.openai-proxy.org/v1'):
        self.client = OpenAI(base_url=base_url, api_key=api_key)

    def fetch_chat_response(self, user_input, model, temperature=0.0):
        # 这里示例中的模型调用现在指定为assistant回应
        current_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"Current Time: {current_time}")
        chat_completion = self.client.chat.completions.create(
            messages=[
                {"role": "assistant", "content": user_input},
            ],
            model=model,
            temperature=temperature,
            top_p = 1.0
        )
        print(f"API Endpoint: {self.client.base_url}")
        print(f"Model Used: {model}")
        print(f"MODEL Response: ",chat_completion)
        return chat_completion


import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


class TextGenerator:
    def __init__(self, model_path):
        # 指定使用显卡1
        self.device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")

        # 初始化 tokenizer 和 model
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="left")
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token  # 设置 pad_token

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map=None  # 不自动映射，手动指定设备
        ).to(self.device)  # 将模型加载到显卡1

    def generate_text(self, messages, document):
        # 格式化输入文本
        formatted_input = self._get_formatted_input(messages, document)
        print(f"Formatted Input:\n{formatted_input}")

        # Tokenize 输入文本并移动到模型设备
        tokenized_prompt = self.tokenizer(
            self.tokenizer.bos_token + formatted_input,
            return_tensors="pt",
            truncation=True,  # 确保输入长度不会超出模型的限制
            max_length=1024  # 控制最大输入长度
        ).to(self.device)  # 将输入数据移动到显卡1

        # 设置结束符
        eos_token_id = self.tokenizer.eos_token_id

        # 使用模型生成文本
        outputs = self.model.generate(
            input_ids=tokenized_prompt.input_ids,
            attention_mask=tokenized_prompt.attention_mask,
            max_new_tokens=256,
            eos_token_id=eos_token_id,
            do_sample=True,  # 控制生成更自然的回答
            temperature=0.6,  # 调节生成的多样性
            top_p=1.0,  # 使用 nucleus sampling
            pad_token_id=self.tokenizer.pad_token_id
        )

        # 解码生成的文本
        response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]
        return self.tokenizer.decode(response, skip_special_tokens=True)

    def _get_formatted_input(self, messages, context):
        # 构建格式化输入
        system_prompt = (
            "System: This is a chat between a user and an artificial intelligence assistant. "
            "The assistant gives helpful, detailed, and polite answers to the user's questions based on the context."
        )
        conversation = '\n\n'.join(
            ["User: " + item["content"] for item in messages if item['role'] == "user"]
        ) + "\n\nAssistant:"
        formatted_input = f"{system_prompt}\n\n{context}\n\n{conversation}"
        return formatted_input
import asyncio
from src.Agents.API_info import ChatActionRunner


class LLMpp:
    def __init__(self):
        self.runner = ChatActionRunner()
        self.question_format = '''You are an assistant responsible for ensuring compliance with the GDPR (General Data Protection Regulation).  
Your task is to carefully identify and redact all personal or sensitive data from the following content while preserving the context and instructions.

Here is the task content, which includes specific questions and instructions:

//{question}//

The categories of data you need to protect include, but are not limited to:
1. **Personal identifiers** (e.g., full names, initials, usernames, or pseudonyms).  
2. **Location data** (e.g., any city, address, or geographical location).  
3. **Contact information** (e.g., phone numbers, email addresses, or mailing addresses).  
4. **Organizational affiliations** (e.g., memberships, employment details, or associations).  

Please redact the necessary personal or sensitive information using the following format:
1. **Personal identifiers** (e.g., full names, initials, usernames, or pseudonyms).  =
   - Format: `<name-1>`, `<name-2>`, etc.
2. **Location data** (e.g., any city, address, or geographical location).  
   - Format: `<location-1>`, `<location-2>`, etc.
3. **Contact information** (e.g., phone numbers, email addresses, or mailing addresses).  
   - Format: `<contact-1>`, `<contact-2>`, etc.
4. **Organizational affiliations** (e.g., memberships, employment details, or associations).  
   - Format: `<organization-1>`, `<organization-2>`, etc.

Return the processed content with sensitive data replaced as per the format above, ensuring the original structure and instructions remain intact.
....
        '''




    async def llm_ppagent(self, question: str):
        try:
            # 确保 question 是字符串
            if not isinstance(question, str):
                raise ValueError("Question must be a string.")

            all_content = self.question_format.format(question=question)

            print('Sub-question generated:', all_content)
            print("----------------------------------------------------------------------------")

            # 执行异步操作
            finally_answer = await self.runner.execute(all_content)
            print("------------------------------------End----------------------------------------")
            return finally_answer
        except Exception as e:
            print(f"An error occurred: {e.__class__.__name__} - {str(e)}")
            return None

import json
import asyncio
from src.Agents.start_Q import QuestionProcessor

async def print_json_details(json_path):
    # 读取 JSONL 文件，每行一个 JSON 对象
    with open(json_path, 'r', encoding='utf-8') as file:
        id_ = 1
        for line in file:
            entry = json.loads(line)  # 解析每一行的 JSON 数据
            # 打印所需信息
            print('---')  # 分隔每个条目
            processor = QuestionProcessor()
            print("Processing question :", entry["idx"])
            print("Question:",entry["inputs"])
            # 注意这里使用 await
            answer = await processor.process_question(entry["inputs"],id_)
            id_ = id_+1
            print("finally answer IS:", answer)
            print(f'targets: {entry["targets"]}')

# 指定 JSON 文件的路径
json_path = '/mnt/c/workspace/code/xagent_zhao/data/logic_grid_puzzle/logic_grid_puzzle_200.jsonl'

# 运行异步函数
if __name__ == '__main__':
    asyncio.run(print_json_details(json_path))
def normalize_sum_to_one(a, b, c):
    """
    Normalizes three numbers so that their sum equals to 1 and their proportions remain the same.

    Parameters:
    a (float): First number.
    b (float): Second number.
    c (float): Third number.

    Returns:
    tuple: A tuple containing the normalized values of a, b, and c.
    """
    total = a + b + c
    if total == 0:
        return 0, 0, 0  # Avoid division by zero if the total is 0
    return a / total, b / total, c / total




import numpy as np
import matplotlib.pyplot as plt

# 定义高斯函数
def gaussian(x, mu, sigma):
    return 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(- (x - mu)**2 / (2 * sigma**2))

# 优化函数以改善标签位置，不让其影响到函数曲线
def plot_gaussians_with_optimized_labels(input_x):
    # 生成x值
    x = np.linspace(0, 1, 1000)

    # 定义标准差
    sigma = 0.05

    # 计算七个不同中点的高斯函数的y值
    y1 = gaussian(x, mu=0.1, sigma=sigma)
    y2 = gaussian(x, mu=0.2, sigma=sigma)
    y3 = gaussian(x, mu=0.3, sigma=sigma)
    y4 = gaussian(x, mu=0.4, sigma=sigma)
    y5 = gaussian(x, mu=0.5, sigma=sigma)
    y6 = gaussian(x, mu=0.6, sigma=sigma)
    y7 = gaussian(x, mu=0.8, sigma=sigma)

    # 生成七个高斯函数在输入x值处的y值
    y_values_at_input = [
        gaussian(input_x, mu=0.1, sigma=sigma),
        gaussian(input_x, mu=0.2, sigma=sigma),
        gaussian(input_x, mu=0.3, sigma=sigma),
        gaussian(input_x, mu=0.4, sigma=sigma),
        gaussian(input_x, mu=0.5, sigma=sigma),
        gaussian(input_x, mu=0.6, sigma=sigma),
        gaussian(input_x, mu=0.8, sigma=sigma)
    ]

    # 绘制图像
    plt.figure(figsize=(10, 5))
    plt.plot(x, y1, label='Mean 0.1')
    plt.plot(x, y2, label='Mean 0.2')
    plt.plot(x, y3, label='Mean 0.3')
    plt.plot(x, y4, label='Mean 0.4')
    plt.plot(x, y5, label='Mean 0.5')
    plt.plot(x, y6, label='Mean 0.6')
    plt.plot(x, y7, label='Mean 0.8')

    # 标出七个点
    plt.scatter([input_x]*7, y_values_at_input, color='red')
    for i, y_val in enumerate(y_values_at_input):
        plt.text(input_x, y_val, f'{y_val:.2f}', horizontalalignment='right', verticalalignment='bottom')

    plt.title('Optimized Gaussian Functions with Labels Not Affecting the Curves')
    plt.xlabel('x')
    plt.ylabel('f(x)')
    plt.grid(True)
    plt.legend(loc='upper right')
    plt.show()

    # 确定最大y值对应的曲线
    max_y_index = y_values_at_input.index(max(y_values_at_input))
    membership = ["Extremely Low", "Low", "Moderately Low", "Medium", "Moderately High", "High", "Extremely High"][max_y_index]

    # 返回隶属度描述
    return membership

# 使用示例
membership = plot_gaussians_with_optimized_labels(0.5)  # 输入x值
print("Membership at x =", 0.5, "is:", membership)


def TF_Google_Weighted_func(a, b):
    number = 0.8
    final_number = number * a + (1 - number) * b
    return final_number
from transformers import pipeline, set_seed
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

def finance_generate_text(text, model_path):
    # 检查CUDA是否可用
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model_path = "C:/Users/zrh/.cache/huggingface/hub/models--lxyuan--distilgpt2-finetuned-finance/snapshots/e185be9dba22a0c041b26293483b55e39461119b"  # 替换为你的模型路径
    # 加载模型
    generator = pipeline("text-generation", model=model_path, device=device.index if device.type == "cuda" else -1)

    # 生成文本
    generated_texts = generator(
        text,
        pad_token_id=generator.tokenizer.eos_token_id,
        max_new_tokens=200,
        num_return_sequences=2
    )

    return generated_texts

def gpt2_generate_text(prompt):
    # 配置GPU设备
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    # 初始化模型和分词器
    tokenizer = GPT2Tokenizer.from_pretrained('C:/Users/zrh/.cache/huggingface/hub/models--distilgpt2\snapshots/38cc92ec43315abd5136313225e95acc5986876c')
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model = GPT2LMHeadModel.from_pretrained('C:/Users/zrh/.cache/huggingface/hub/models--FredZhang7--distilgpt2-stable-diffusion-v2/snapshots/f839bc9217d4bc3694e4c5285934b5e671012f85').to(device)

    temperature = 0.9
    top_k = 8
    max_length = 80
    repetition_penalty = 1.2
    num_return_sequences = 5

    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(device)

    # 在GPU上运行生成
    output = model.generate(
        input_ids,
        do_sample=True,
        temperature=temperature,
        top_k=top_k,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        repetition_penalty=repetition_penalty,
        penalty_alpha=0.6,
        no_repeat_ngram_size=1,
        early_stopping=True
    )

    results = []
    for i in range(len(output)):
        results.append(tokenizer.decode(output[i], skip_special_tokens=True))

    return results



def legal_generate_text(text):
    import torch
    if torch.cuda.is_available():
        device = 0
    else:
        device = -1

    set_seed(42)

    # 修改这里来从本地加载模型
    model_path = 'C:/Users/zrh/.cache/huggingface/hub/models--umarbutler--open-australian-legal-distilgpt2/snapshots/06071d63a090272147154ebaf04f61df471cdbcf'  # 你的本地模型路径
    generator = pipeline('text-generation', model=model_path, device=device)

    generated_text = generator(text, max_length=50, num_return_sequences=5)
    generated_text = [result['generated_text'] for result in generated_text]

    return generated_text


def medical_generation(text):
    import torch
    if torch.cuda.is_available():
        device = 0
    else:
        device = -1

    set_seed(42)

    # 修改这里来从本地加载模型
    model_path = 'C:/Users/zrh/.cache/huggingface/hub/distilgpt2-finetuned-medical'  # 你的本地模型路径
    generator = pipeline('text-generation', model=model_path, device=device)

    generated_text = generator(text, max_length=50, num_return_sequences=5)
    generated_text = [result['generated_text'] for result in generated_text]

    return generated_text
'''
 

text = "Your input text here"
generated_texts = generate_text(text)
print(generated_texts)
'''
from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
import json
import torch
local_model_path = "/home/yang/home/yang/workspace/code/xagent/model/NER"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(local_model_path)
model = AutoModelForTokenClassification.from_pretrained(local_model_path)
model.to(device)

nlp = pipeline("ner", model=model, tokenizer=tokenizer)

def find_nth_word_position(sentence, n):
    """
    Find the position of the nth word in a sentence.

    Parameters:
    sentence (str): The sentence in which to search for the word.
    n (int): The position of the word to find (1-based indexing).

    Returns:
    int: The starting index of the nth word in the sentence, or -1 if not found.
    """
    words = sentence.split()  # Split the sentence into words

    if n > len(words) or n < 1:  # Check if the request is out of bounds
        return -1  # Return -1 if the nth word doesn't exist

    # Find the start position of the nth word
    start_position = sum(len(word) + 1 for word in words[:n - 1])

    return start_position

# 模拟模型处理函数
def process_context(context):
    # 这里可以是你调用模型的代码，对context进行处理
    # 替换成你的实际处理逻辑
    processed_result = context
    return processed_result

TP = 0
FN = 0
TN = 0
FP = 0
# 读取测试集文件
with open('/home/yang/home/yang/workspace/code/xagent/doc/mrc-ner.test', 'r', encoding='utf-8') as file:
    test_set = json.load(file)

# 对每个样本进行处理并输出结果
for sample in test_set:
    qas_id = sample["qas_id"]
    contexts = sample["context"]
    processed_context = process_context(contexts)
    print("例子")
    print(sample)
    # 输出结果


    ner_results = nlp(processed_context)
    print("处理后")
    print(ner_results)
    if not sample['impossible']:
        # 检查是否有符合条件的entity
        entity_found = False
        for ner_result in ner_results:
            print("内容,NER")
            print(ner_result)
            if ner_result['entity'] in ['B-' + sample['entity_label'], 'I-' + sample['entity_label']]:
                print("结果有有有有有有有有有有有有有有有有有有有")
                print(find_nth_word_position(sample['context'],sample['start_position'][0]))
                print(ner_result['start'])
                if len(sample['start_position']) > 0 and find_nth_word_position(sample['context'],sample['start_position'][0]+1) == ner_result['start']:
                    entity_found = True
                    break
                elif len(sample['start_position']) > 1 and find_nth_word_position(sample['context'],sample['start_position'][1]+1) == ner_result['start']:
                    entity_found = True
                    break
        if entity_found:
            TP += 1
        if not entity_found:
            FN += 1
    else:
        # 检查是否有不应存在的entity
        entity_found = False
        for ner_result in ner_results:
            if ner_result['entity'] in ['B-' + sample['entity_label'], 'I-' + sample['entity_label']]:
                FP += 1
                entity_found = True
                break
        if not entity_found:
            TN += 1

    # 输出结果
    print(f"TP: {TP}, FN: {FN}, FP: {FP}, TN: {TN}")


















from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
import re

class EntityEncryptor:
    def __init__(self, model_path, tokenizer_path, device=1):
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path
        self.device = device

        # 加载模型和tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_path)
        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)

        # 初始化NER pipeline
        self.nlp = pipeline("ner", model=self.model, tokenizer=self.tokenizer, device=self.device)

    def remove_subwords(self, entity_list):
        """去除实体中的冗余子串"""
        words = set(entity['word'] for entity in entity_list)
        words_to_remove = {word for word in words if any(word != other and word in other for other in words)}
        return [entity for entity in entity_list if entity['word'] not in words_to_remove]

    def long_entities(self, text):
        """处理长度超过512的文本"""
        sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)
        all_entities = []
        offset = 0

        for sentence in sentences:
            ner_results = self.nlp(sentence)
            for entity in ner_results:
                entity['start'] += offset
                entity['end'] += offset
                all_entities.append(entity)
            offset += len(sentence) + 1

        return all_entities

    def replace_and_record(self, text, ner_results, entity_type, placeholder_prefix, replacement_dict):
        """替换实体并记录替换关系"""
        unique_entities = {result['word'] for result in ner_results if result['entity'] == entity_type}
        for i, entity in enumerate(sorted(unique_entities), start=1):
            placeholder = f'<{placeholder_prefix}-{i}>'
            text = text.replace(entity, placeholder)
            replacement_dict[entity] = placeholder
        return text

    def encrypt_entities(self, text):
        """对文本进行实体加密"""
        tokens = text.split()
        if len(tokens) < 512:
            ner_results = self.nlp(text)
        else:
            print(f"文本大于512字符，共 {len(tokens)} 个token")
            ner_results = self.long_entities(text)

        ner_results = self.remove_subwords(ner_results)
        entities_to_remove = [r for r in ner_results if r['entity'].startswith(('I-ORG', 'I-PER', 'I-LOC'))]

        # 删除指定实体并调整文本
        for entity in sorted(entities_to_remove, key=lambda x: x['start'], reverse=True):
            before_entity = text[:entity['start']].rstrip()
            after_entity = text[entity['end']:].lstrip()
            text = before_entity + (' ' if before_entity and after_entity else '') + after_entity

        replacement_dict = {}
        text = self.replace_and_record(text, ner_results, 'B-PER', 'name', replacement_dict)
        text = self.replace_and_record(text, ner_results, 'B-LOC', 'location', replacement_dict)
        text = self.replace_and_record(text, ner_results, 'B-ORG', 'organization', replacement_dict)

        return text, replacement_dict



"""# 实例化类
model_path = "/home/zhaorh/code/ppagent//model/NER"
tokenizer_path = "/home/zhaorh/code/ppagent//model/NER"
entity_encryptor = EntityEncryptor(model_path, tokenizer_path)

# 测试加密功能
text = "John works at OpenAI in San Francisco."
encrypted_text, replacement_dict = entity_encryptor.encrypt_entities(text)

print("加密后的文本:", encrypted_text)
print("替换关系:", replacement_dict)"""
import json


def find_data_by_id(file_path, target_id):
    """
    从JSON文件中查找特定ID的数据并返回相关信息

    参数:
    file_path (str): JSON文件路径
    target_id (str): 要查找的ID

    返回:
    dict: 包含问题、答案、难度级别和所有上下文文本的字典
    """
    # 初始化一个变量用来存储找到的数据
    data_found = None
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        for item in data:
            if item['id'] == target_id:
                data_found = item  # 将找到的数据赋值给data_found变量
                break

    if data_found is None:
        return None  # 如果没有找到数据，返回None

    # 提取问题、答案、难度级别
    result = {
        "Question": data_found['Question'],
        "Answer": data_found['Answer'],
        "Difficulty Level": data_found['Difficulty Level'],
        "All Context Texts": data_found['All Context Texts']
    }



    return result



import json

def extract_ids(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = json.load(file)
        id_list = [item['id'] for item in data]
    return id_list

import re


def hide_phone_numbers(text):
    # 通过正则表达式隐藏电话号码中间的数字
    return re.sub(r'\d{3}-\d{4}-(\d{4})', r'***-****-\1', text)

def hide_email_addresses(text):
    # 通过正则表达式隐藏邮箱用户名部分
    return re.sub(r'(\w+)(@\w+\.\w+)', r'***\2', text)

def hide_id_numbers(text):
    # 通过正则表达式隐藏身份证号码的部分数字
    return re.sub(r'\d{6}(?:19|20)\d{2}\d{2}\d{2}\d{2}\d{1}', r'**************\\d', text)

def hide_card_numbers(sentence):
    # 正则表达式模式，用于匹配银行卡号（16位数字）
    card_number_pattern = r'\d{16}'

    # 使用正则表达式查找句子中的银行卡号
    card_numbers = re.findall(card_number_pattern, sentence)

    # 将句子中的银行卡号替换为星号(*)，保留首尾各4位数字
    for card_number in card_numbers:
        masked_card_number = card_number[:4] + '*' * 8 + card_number[-4:]
        sentence = sentence.replace(card_number, masked_card_number)

    return sentence


def hide_ip_addresses(sentence):
    # 正则表达式模式，用于匹配IPv4地址
    ip_address_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'

    # 使用正则表达式查找句子中的所有IP地址
    ip_addresses = re.findall(ip_address_pattern, sentence)

    # 将句子中的IP地址替换为星号(*)
    for ip_address in ip_addresses:
        masked_ip_address = '*' * len(ip_address)
        sentence = sentence.replace(ip_address, masked_ip_address)

    return sentenceimport json

def save_data_to_json(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as file:
        json.dump(data, file, ensure_ascii=False, indent=4)


import json
import asyncio
import aiofiles
from src.Agents.start_Q import QuestionProcessor
from src.Agents.memory import config_w2v
from src.Agents.privacy_part import privacy_part
config_w2v.initialize_calculator()
from src.LLM_pp_agent import LLMpp
import re
from src.Evaluation import *

import re


async def process_answers(task_all, line_number, data):
    processor = QuestionProcessor()
    plan_answer, answers, final_answer = await processor.process_question(task_all, line_number)

    print('Fusion final answer:', plan_answer)
    print('SUB ANSWER:', answers)
    print('Repair fusion final answer:', final_answer)

    sub_a = " ".join(answers)
    print('+++++sub_question++++++')
    answer_plan_matches_sub = count_nested_answers_in_text(sub_a, data['answers'])
    print(f"SUB Number of plan answers found in story: {answer_plan_matches_sub}")

    print('+++++plan_question++++++')
    answer_plan_matches = count_nested_answers_in_text(plan_answer, data['answers'])
    print(f"Number of fusion answers found in story: {answer_plan_matches}")

    if "perfect" in final_answer.lower():  # Check for 'perfect' case-insensitively
        print('+++++plan_repair_question++++++')
        answer_repair_matches = answer_plan_matches
    else:
        print('+++++plan_repair_question++++++')
        answer_repair_matches = count_nested_answers_in_text(final_answer, data['answers'])
        print(f"Number of fusion answers found in story: {answer_repair_matches}")

    return answer_plan_matches, answer_repair_matches, answer_plan_matches_sub


async def read_json_file(file_path):
    llm_bleu_scores = []
    llm_semantic_similarities = []
    ner_bleu_scores = []
    ner_semantic_similarities = []
    mask_bleu_scores = []
    mask_semantic_similarities = []
    scores_ner = dict(name=0, location=0, organization=0, email_id=0)
    scores_2_ner = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_llm = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_2_llm = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_mask  = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_2_mask = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    missing_count_ner_sum = 0
    non_privacy_count_ner_sum = 0
    missing_count_llm_sum = 0
    non_privacy_count_llm_sum = 0
    missing_count_mask_sum = 0
    non_privacy_count_mask_sum = 0

    runner = ChatActionRunner()
    """
    读取 JSON 文件，对每个样本执行处理，并在用户按 Enter 确认后继续。
    """
    try:
        start_line = 0
        sum1 =0
        sum2 =0
        sum3 =0
        sum4 =0
        sum5 =0
        sum6 =0

        async with aiofiles.open(file_path, mode='r', encoding='utf-8') as f:
            content = await f.read()
            data_list = json.loads(content)
            for line_number, data in enumerate(data_list, start=1):
                if line_number < start_line:
                    continue
                print('id:', data.get('id'))
                print('question:', data.get('question'))
                email = data.get('question')
                print('answers:', data.get('answers'))
                print('privacy_data:', data.get('privacy_data'))
                print('-----------------------------------')
                task = 'Write a reply to the letter by answering the questions in the form of a story, below:\n{email}'
                task_all = task.format(email=email)
                print(data['answers'])
                """NER_question, replacement_dict, masked_question = await privacy_part(task_all)
                # 实例化 LLMpp 类
                llm_instance = LLMpp()
                LLM_masked_question = await llm_instance.llm_ppagent(task_all)"""

                """(
                    scores_ner, scores_2_ner,
                    scores_llm, scores_2_llm,
                    scores_mask, scores_2_mask,
                    ner_bleu_scores, ner_semantic_similarities,
                    llm_bleu_scores, llm_semantic_similarities,
                    mask_bleu_scores, mask_semantic_similarities,
                    missing_count_ner_sum, non_privacy_count_ner_sum,
                    missing_count_llm_sum, non_privacy_count_llm_sum,
                    missing_count_mask_sum, non_privacy_count_mask_sum
                ) = process_privacy_scores_and_metrics(
                    scores_ner, scores_2_ner,
                    scores_llm, scores_2_llm,
                    scores_mask, scores_2_mask,
                    NER_question, LLM_masked_question, masked_question,
                    data, task_all,
                    ner_bleu_scores, ner_semantic_similarities,
                    llm_bleu_scores, llm_semantic_similarities,
                    mask_bleu_scores, mask_semantic_similarities,
                    missing_count_ner_sum, non_privacy_count_ner_sum,
                    missing_count_llm_sum, non_privacy_count_llm_sum,
                    missing_count_mask_sum, non_privacy_count_mask_sum
                )"""

                answer_plan_matches, answer_repair_matches, answer_plan_matches_sub = await process_answers(task_all,
                                                                                                          line_number,
                                                                                                            data)
                #ner_answer = await runner.execute(NER_question)
                #answer_ner_matches = count_nested_answers_in_text(ner_answer, data['answers'])
                #llm_answer =await runner.execute(LLM_masked_question)
                #answer_llm_matches = count_nested_answers_in_text(llm_answer, data['answers'])
                base_answer = await runner.execute(task_all)
                answer_base_matches = count_nested_answers_in_text(base_answer, data['answers'])
                sum1 = answer_plan_matches + sum1
                sum2 = answer_repair_matches + sum2
                sum3 = answer_plan_matches_sub + sum3
                #sum4 = answer_ner_matches +sum4
                #sum5 = answer_llm_matches +sum5
                sum6 = answer_base_matches+sum6
                print('my: The number of correct answers to the verification sub_questions:', sum3)
                print('my:The number of correct answers after the repair:', sum2)
                print('my:The number of correct answers:', sum1)
                #print("ner:",sum4)
                #print("llm:",sum5)
                print("base:",sum6)

            """calculate_and_print_metrics(
                scores_ner, scores_2_ner, missing_count_ner_sum, non_privacy_count_ner_sum,
                scores_llm, scores_2_llm, missing_count_llm_sum, non_privacy_count_llm_sum,
                scores_mask, scores_2_mask, missing_count_mask_sum, non_privacy_count_mask_sum,
                ner_bleu_scores, ner_semantic_similarities,
                llm_bleu_scores, llm_semantic_similarities,
                mask_bleu_scores, mask_semantic_similarities
            )"""


    except Exception as e:
        print(f"An error occurred: {e}")

# 设置开始处理的样本序号


# 调用异步函数
asyncio.run(read_json_file('/home/zhaorh/code/ppagent/data/wki/HotpotQA/emails.json'))

import json
import asyncio
import aiofiles
from src.Agents.start_Q import QuestionProcessor
from src.Agents.memory import config_w2v
from src.Agents.privacy_part import privacy_part
config_w2v.initialize_calculator()
from src.LLM_pp_agent import LLMpp
import re
from src.Evaluation import *
from src.Agents.memory import config_w2v
from src.Agents.API_info import ChatActionRunner
config_w2v.initialize_calculator()

import re

def evaluate_answer(text, standard_answer):
    # 如果 text 是列表，将其转换为一个字符串
    print(text)
    if isinstance(text, list):
        text = ' '.join(map(str, text))  # 将列表元素连接为一个字符串

    # 确保 text 和 standard_answer 都是字符串类型，并忽略大小写
    text_str = str(text).strip().lower()
    standard_answer_str = str(standard_answer).strip().lower()

    # 使用正则表达式来匹配标准答案作为完整单词出现
    if re.search(rf'\b{re.escape(standard_answer_str)}\b', text_str):
        return 1
    else:
        return 0


async def process_answers(task_all, line_number, data):
    processor = QuestionProcessor()
    plan_answer, answers, final_answer = await processor.process_question(task_all, line_number)

    print('Fusion final answer:', plan_answer)
    print('SUB ANSWER:', answers)
    print('Repair fusion final answer:', final_answer)

    sub_a = " ".join(answers)
    print('+++++sub_question++++++')
    answer_plan_matches_sub = count_nested_answers_in_text(sub_a, data['answers'])
    print(f"SUB Number of plan answers found in story: {answer_plan_matches_sub}")

    print('+++++plan_question++++++')
    answer_plan_matches = count_nested_answers_in_text(plan_answer, data['answers'])
    print(f"Number of fusion answers found in story: {answer_plan_matches}")

    if "perfect" in final_answer.lower():  # Check for 'perfect' case-insensitively
        print('+++++plan_repair_question++++++')
        answer_repair_matches = answer_plan_matches
    else:
        print('+++++plan_repair_question++++++')
        answer_repair_matches = count_nested_answers_in_text(final_answer, data['answers'])
        print(f"Number of fusion answers found in story: {answer_repair_matches}")

    return answer_plan_matches, answer_repair_matches, answer_plan_matches_sub


async def read_json_file(file_path):
    llm_bleu_scores = []
    llm_semantic_similarities = []
    ner_bleu_scores = []
    ner_semantic_similarities = []
    mask_bleu_scores = []
    mask_semantic_similarities = []
    scores_ner = dict(name=0, location=0, organization=0, email_id=0)
    scores_2_ner = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_llm = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_2_llm = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_mask  = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    scores_2_mask = {'name': 0, 'location': 0, 'organization': 0, 'email_id': 0}
    missing_count_ner_sum = 0
    non_privacy_count_ner_sum = 0
    missing_count_llm_sum = 0
    non_privacy_count_llm_sum = 0
    missing_count_mask_sum = 0
    non_privacy_count_mask_sum = 0

    runner = ChatActionRunner()
    """
    读取 JSON 文件，对每个样本执行处理，并在用户按 Enter 确认后继续。
    """
    try:
        start_line = 1
        sum1 =0
        sum2 =0
        sum3 =0
        sum4 =0
        sum5 =0
        sum6 =0

        async with aiofiles.open(file_path, mode='r', encoding='utf-8') as f:
            content = await f.read()
            data_list = json.loads(content)
            for line_number, data in enumerate(data_list, start=1):
                if line_number < start_line:
                    continue
                print('id:', data.get('id'))
                print('question:', data.get('passage'))
                email = data.get('passage')
                print('answers:', data.get('answer'))
                print('privacy_data:', data.get('privacy_data'))
                print('-----------------------------------')
                task = '''Please respond to the following letter by choosing either “True” or “False”:
                        - If your answer is “True,” ensure the reply contains no mention of “False” anywhere.
                        - If your answer is “False,” ensure the reply contains no mention of “True” anywhere. below:\n{email}'''
                task_all = task.format(email=email)
                print(data['answer'])
                NER_question, replacement_dict, masked_question = await privacy_part(task_all)
                """# 实例化 LLMpp 类
                llm_instance = LLMpp()
                LLM_masked_question = await llm_instance.llm_ppagent(task_all)

                (
                    scores_ner, scores_2_ner,
                    scores_llm, scores_2_llm,
                    scores_mask, scores_2_mask,
                    ner_bleu_scores, ner_semantic_similarities,
                    llm_bleu_scores, llm_semantic_similarities,
                    mask_bleu_scores, mask_semantic_similarities,
                    missing_count_ner_sum, non_privacy_count_ner_sum,
                    missing_count_llm_sum, non_privacy_count_llm_sum,
                    missing_count_mask_sum, non_privacy_count_mask_sum
                ) = process_privacy_scores_and_metrics(
                    scores_ner, scores_2_ner,
                    scores_llm, scores_2_llm,
                    scores_mask, scores_2_mask,
                    NER_question, LLM_masked_question, masked_question,
                    data, task_all,
                    ner_bleu_scores, ner_semantic_similarities,
                    llm_bleu_scores, llm_semantic_similarities,
                    mask_bleu_scores, mask_semantic_similarities,
                    missing_count_ner_sum, non_privacy_count_ner_sum,
                    missing_count_llm_sum, non_privacy_count_llm_sum,
                    missing_count_mask_sum, non_privacy_count_mask_sum
                )"""
                processor = QuestionProcessor()
                plan_answer, answers, final_answer = await processor.process_question(masked_question, line_number)
                #ner_answer = await runner.execute(NER_question)
                #llm_answer =await runner.execute(LLM_masked_question)
                base_answer = await runner.execute(task_all)

                if "perfect" in final_answer.lower():
                    final_answer = plan_answer
                standard_answer = data.get('answer')
                print("标准答案",standard_answer)
                # 初始化各个 sum 变量
                sum11 = evaluate_answer(plan_answer, standard_answer)
                sum22 = evaluate_answer(final_answer, standard_answer)
                sum33 = evaluate_answer(answers, standard_answer)
                #sum44 = evaluate_answer(ner_answer, standard_answer)
                #sum55 = evaluate_answer(llm_answer, standard_answer)
                sum66 = evaluate_answer(base_answer, standard_answer)
                sum1 = sum11 + sum1
                sum2 = sum22 + sum2
                sum3 = sum33 + sum3
                #sum4 = sum44 + sum4
                #sum5 = sum55 + sum5
                sum6 = sum66 + sum6
                print('my:The number of correct answers after the repair:',final_answer)
                print('my:The number of correct answers:', plan_answer)
                print('my: The number of correct answers to the verification sub_questions:', sum3)
                print('my:The number of correct answers after the repair:', sum2)
                print('my:The number of correct answers:', sum1)
                #print("ner:",sum4)
                #print("llm:",sum5)
                print("base:",sum6)
            calculate_and_print_metrics(
                scores_ner, scores_2_ner, missing_count_ner_sum, non_privacy_count_ner_sum,
                scores_llm, scores_2_llm, missing_count_llm_sum, non_privacy_count_llm_sum,
                scores_mask, scores_2_mask, missing_count_mask_sum, non_privacy_count_mask_sum,
                ner_bleu_scores, ner_semantic_similarities,
                llm_bleu_scores, llm_semantic_similarities,
                mask_bleu_scores, mask_semantic_similarities
            )

    except Exception as e:
        print(f"An error occurred: {e}")

# 设置开始处理的样本序号


# 调用异步函数
asyncio.run(read_json_file('/home/zhaorh/code/ppagent/data/pp_data/reasoning_pp.json'))


def weighted_metrics(hidden_count, total_privacy, false_hidden, non_privacy_count):
    # 计算隐私类的Precision、Recall和F1
    if hidden_count + false_hidden > 0:
        precision_privacy = hidden_count / (hidden_count + false_hidden)
    else:
        precision_privacy = 0.0

    if total_privacy > 0:
        recall_privacy = hidden_count / total_privacy
    else:
        recall_privacy = 0.0

    if precision_privacy + recall_privacy > 0:
        f1_privacy = 2 * precision_privacy * recall_privacy / (precision_privacy + recall_privacy)
    else:
        f1_privacy = 0.0

    # 计算非隐私类的Precision、Recall和F1
    if non_privacy_count > 0:
        precision_non_privacy = (non_privacy_count - false_hidden) / non_privacy_count
        recall_non_privacy = (non_privacy_count - false_hidden) / non_privacy_count
    else:
        precision_non_privacy = 0.0
        recall_non_privacy = 0.0

    if precision_non_privacy + recall_non_privacy > 0:
        f1_non_privacy = 2 * precision_non_privacy * recall_non_privacy / (precision_non_privacy + recall_non_privacy)
    else:
        f1_non_privacy = 0.0

    # 计算权重
    total_count = total_privacy + non_privacy_count
    weight_privacy = total_privacy / total_count if total_count > 0 else 0.0
    weight_non_privacy = non_privacy_count / total_count if total_count > 0 else 0.0

    # 加权Precision、Recall和F1
    weighted_precision = (precision_privacy * weight_privacy) + (precision_non_privacy * weight_non_privacy)
    weighted_recall = (recall_privacy * weight_privacy) + (recall_non_privacy * weight_non_privacy)
    if weighted_precision + weighted_recall > 0:
        weighted_f1 = 2 * weighted_precision * weighted_recall / (weighted_precision + weighted_recall)
    else:
        weighted_f1 = 0.0

    return weighted_precision, weighted_recall, weighted_f1

# 定义输入数据
datasets = {
    "NER": {
        "hidden_count": 405,
        "total_privacy": 500,
        "false_hidden": 1995,
        "non_privacy_count": 12626,
    },
    "LLM": {
        "hidden_count": 500,
        "total_privacy": 500,
        "false_hidden": 1190,
        "non_privacy_count": 13119,
    },
    "Mask": {
        "hidden_count": 468,
        "total_privacy": 500,
        "false_hidden": 632,
        "non_privacy_count": 13158,
    },
}

# 计算并打印结果
for dataset_name, metrics in datasets.items():
    weighted_precision, weighted_recall, weighted_f1 = weighted_metrics(
        metrics["hidden_count"],
        metrics["total_privacy"],
        metrics["false_hidden"],
        metrics["non_privacy_count"]
    )
    print(f"{dataset_name} Metrics:")
    print(f"  Weighted Precision: {weighted_precision:.4f}")
    print(f"  Weighted Recall: {weighted_recall:.4f}")
    print(f"  Weighted F1 Score: {weighted_f1:.4f}")
    print("-" * 50)


from concurrent.futures import ThreadPoolExecutor
from gensim.models import KeyedVectors
from nltk.tokenize import word_tokenize
import numpy as np


class SimilarityCalculator:
    def __init__(self):
        """
        初始化类，加载预训练的词向量模型，并设置目标词列表。

        参数:
        - model_path: 预训练模型的路径 (例如 Google News Word2Vec 模型)。
        - target_words: 需要计算相似度的目标词列表。
        """
        self.model_path = '/mnt/c/workspace/code/xagent_zhao_Llama3/model/gensim-data/word2vec-google-news-300/word2vec-google-news-300/GoogleNews-vectors-negative300.bin'
        self.target_words = ["Entertainment", "financial", "History", "legal", "Literature", "medical", "Politics", "Sports",
                        "technology", "science"]
        # 加载预训练的 Word2Vec 模型
        self.model = KeyedVectors.load_word2vec_format(self.model_path, binary=True)
        self.model.fill_norms()

        # 设置目标词列表
        self.target_words = self.target_words

    def preprocess_text(self, text):
        """
        预处理文本，将句子分解为单词。

        参数:
        - text: 输入文本，可能是一个词或一个句子。

        返回:
        - 分词后的单词列表。
        """
        words = word_tokenize(text.lower())
        return [word for word in words if word.isalpha()]

    def calculate_similarity(self, word_or_sentence):
        """
        计算一个词或句子与目标词列表中每个词的相似度分数。

        参数:
        - word_or_sentence: 输入，可以是一个词或一个句子。

        返回:
        - 一个字典，包含每个目标词与输入词/句的最高相似度。
        """
        try:
            # 预处理输入，将句子或词分解为单词
            words = self.preprocess_text(word_or_sentence)

            # 初始化相似度结果
            similarity_scores = {}

            # 使用并行计算
            with ThreadPoolExecutor() as executor:
                # 创建任务列表，针对每个目标词执行并行计算
                future_to_target = {executor.submit(self._calculate_max_similarity, words, target_word): target_word for
                                    target_word in self.target_words}

                # 获取结果并存入 similarity_scores 字典
                for future in future_to_target:
                    target_word = future_to_target[future]
                    try:
                        max_similarity = future.result()
                        similarity_scores[target_word] = max_similarity
                    except Exception as e:
                        print(f"Error calculating similarity for '{target_word}': {e}")
                        similarity_scores[target_word] = None

            return similarity_scores

        except Exception as e:
            print(f"An error occurred while calculating similarity:{e}")
            return None

    def _calculate_max_similarity(self, words, target_word):
        """
        辅助函数，计算输入词列表中与目标词的最高相似度。

        参数:
        - words: 输入词列表（句子切分后的词）。
        - target_word: 目标词。

        返回:
        - 输入词列表与目标词的最高相似度。
        """
        if target_word not in self.model.key_to_index:
            print(f"Target word '{target_word}' is not in vocabulary and has been skipped.")
            return None

        max_similarity = 0
        for w in words:
            if w in self.model.key_to_index:
                similarity = self.model.similarity(w, target_word)
                if similarity > max_similarity:
                    max_similarity = similarity
            else:
                print(f"The word '{w}' is not in the vocabulary and has been skipped.")

        return max_similarity if max_similarity > 0 else None


import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import numpy as np
import os
import asyncio
import aiofiles

class TextDomainAnalyzer:
    def __init__(self):
        # 初始化停用词
        self.vectorizer = None
        self.stop_words = set(stopwords.words('english'))

    def preprocess_text(self, text):
        """
        预处理给定文本：分词、转小写、移除非字母字符、过滤停用词。
        """
        words = nltk.word_tokenize(text)
        words = [word.lower() for word in words if word.isalpha()]
        words = [word for word in words if word not in self.stop_words]
        return ' '.join(words)

    async def domain_membership_score_normalized(self, text, domain):
        """
        计算文本相对于给定领域的归属分数。
        """
        # 文本预处理
        processed_text = self.preprocess_text(text)

        # 加载领域特定关键词
        keyword_file_path = f'../../data/field/{domain}_keywords.txt'
        if not os.path.exists(keyword_file_path):
            return domain, 0  # 如果文件不存在，返回0分

        async with aiofiles.open(keyword_file_path, 'r') as file:
            domain_keywords = await file.read()
            domain_keywords = [keyword.lower().strip() for keyword in domain_keywords.splitlines()]

        # 初始化向量化器
        self.vectorizer = TfidfVectorizer()
        self.vectorizer.fit([processed_text] + domain_keywords)

        # 转换文本到TF-IDF矩阵
        tfidf_matrix = self.vectorizer.transform([processed_text]).toarray()

        # 计算领域关键词的TF-IDF分数
        domain_scores = [tfidf_matrix[0, self.vectorizer.vocabulary_.get(keyword)] for keyword in domain_keywords if
                         keyword in self.vectorizer.vocabulary_]
        total_domain_score = np.sum(domain_scores)

        # 计算所有词汇的总分数
        total_score = np.sum(tfidf_matrix[0])

        # 归一化领域得分
        normalized_score = total_domain_score / total_score if total_score > 0 else 0
        return domain, normalized_score
import numpy as np
#from src.Agents.memory import config_w2v
#config_w2v.initialize_calculator()
class SimilarityMatrixCalculator:
    def __init__(self, result, calculator):
        """
        :param result: 输入的领域词列表，形式如[('Entertainment', 'Extremely High'), ...]
        :param calculator: 相似度计算器实例
        """
        self.result = result
        self.calculator = calculator
        self.domains = ["Entertainment", "financial", "History", "legal", "Literature",
                        "medical", "Politics", "Sports", "technology", "science"]
        self.similarity_matrix = None

    def calculate_matrix(self):
        """
        计算相似度矩阵
        """
        self.similarity_matrix = np.zeros((len(self.result), len(self.domains)))

        for i, (word, _) in enumerate(self.result):
            # 如果 word 包含多个词，则选择每个词与目标词的最高相似度
            if ' ' in word:
                words = word.split()  # 将包含多个词的字符串分开
                max_similarity_dict = {}

                for w in words:
                    similarity = self.calculator.calculate_similarity(w)

                    if isinstance(similarity, dict):
                        for domain, score in similarity.items():
                            if score is None:
                                score = 0  # 将 None 设为 0
                            if domain not in max_similarity_dict or (score > max_similarity_dict[domain]):
                                max_similarity_dict[domain] = score
                    else:
                        print(f"Warning: Similarity value for '{w}' is not a dictionary. Using default value 0.")

                similarity = max_similarity_dict
            else:
                # 使用相似度计算器计算相似度
                similarity = self.calculator.calculate_similarity(word)

                if similarity is None:
                    similarity = {}  # 如果 similarity 为 None，则使用空字典

            # 检查返回的类型，并输出调试信息
            print(f"Similarity between {word} and domains: {similarity} (Type: {type(similarity)})")

            # 如果返回的是字典，提取每个领域的相似度值
            if isinstance(similarity, dict):
                for j, domain in enumerate(self.domains):
                    if domain in similarity:
                        self.similarity_matrix[i, j] = similarity[domain]
                    else:
                        print(
                            f"Warning: Domain '{domain}' not found in similarity dictionary for word '{word}'. Using default value 0.")
                        self.similarity_matrix[i, j] = 0
            else:
                # 如果返回的不是字典，抛出警告并使用默认值
                print(f"Warning: Similarity value for {word} is not a dictionary. Using default value 0.")
                self.similarity_matrix[i, :] = 0

        return self.similarity_matrix

    def return_matrix(self):
        """
        打印相似度矩阵
        """
        if self.similarity_matrix is not None:
            return self.similarity_matrix



"""
# 输入的领域词列表
result = [('Entertainment', 'Extremely High'), ('Music and it', 'High'), ('Animation or get', 'High')]

matrix_calculator = SimilarityMatrixCalculator(result, config_w2v.calculator)

# 计算并打印相似度矩阵
matrix_calculator.calculate_matrix()
similarity_matrix = matrix_calculator.return_matrix()
print(similarity_matrix)
"""
